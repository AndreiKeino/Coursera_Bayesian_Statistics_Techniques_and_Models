WEBVTT

1
00:00:00.046 --> 00:00:03.641
[MUSIC]

2
00:00:03.641 --> 00:00:07.207
In lesson one,
we defined a statistical model.

3
00:00:07.207 --> 00:00:10.662
As a mathematical structure
used to imitate or

4
00:00:10.662 --> 00:00:13.947
approximate the data generating process.

5
00:00:13.947 --> 00:00:16.165
It incorporates uncertainty and

6
00:00:16.165 --> 00:00:19.339
variability using
the theory of probability.

7
00:00:20.920 --> 00:00:24.170
A model could be very simple,
involving only one variable.

8
00:00:25.370 --> 00:00:33.213
For example, suppose our data consists
of the heights of n=15 adult men.

9
00:00:35.652 --> 00:00:39.785
So we have the heights of n=15 men.

10
00:00:43.035 --> 00:00:45.798
Clearly it would be very expensive or

11
00:00:45.798 --> 00:00:49.907
even impossible to collect
the genetic information.

12
00:00:49.907 --> 00:00:54.290
That fully explains the variability
in these men's heights.

13
00:00:54.290 --> 00:00:58.370
We only have the height
measurements available to us.

14
00:00:58.370 --> 00:01:00.134
To account for the variability,

15
00:01:00.134 --> 00:01:04.850
we might assume that the men's
heights follow a normal distribution.

16
00:01:04.850 --> 00:01:09.980
So we could write the model like this, yi

17
00:01:09.980 --> 00:01:15.470
will represent the height for
person i, i will be our index.

18
00:01:16.640 --> 00:01:21.771
This will be equal to a constant,
a number mu which

19
00:01:21.771 --> 00:01:27.159
will represents the mean for
all men plus epsilon I.

20
00:01:27.159 --> 00:01:32.271
This is the individual error term for
individual i.

21
00:01:32.271 --> 00:01:37.253
We're going to assume that
epsilon i comes from a normal

22
00:01:37.253 --> 00:01:42.460
distribution with mean zero and
variance sigma squared.

23
00:01:42.460 --> 00:01:47.950
We are also going to assume that
these epsilons are independent and

24
00:01:47.950 --> 00:01:51.220
identically distributed from
this normal distribution.

25
00:01:53.180 --> 00:01:59.772
This is also for i equal to 1 up
to n which will be 15 in our case.

26
00:01:59.772 --> 00:02:05.476
Equivalently we could write this
model directly for the yi themselves.

27
00:02:05.476 --> 00:02:11.100
So each yi comes from a normal
distribution independent and

28
00:02:11.100 --> 00:02:16.618
identically distributed with
the normal distribution.

29
00:02:16.618 --> 00:02:20.110
With mean mu and variance sigma squared.

30
00:02:22.180 --> 00:02:27.944
This specifies a probability
distribution and a model for the data.

31
00:02:27.944 --> 00:02:32.550
If we know the values of mu and sigma.

32
00:02:32.550 --> 00:02:37.126
It also suggests how we might
generate more fake data that behaves

33
00:02:37.126 --> 00:02:39.710
similarly to our original data set.

34
00:02:41.090 --> 00:02:45.490
A model can be as simple as the one
right here or as complicated and

35
00:02:45.490 --> 00:02:49.740
sophisticated as we need to
capture the behavior of the data.

36
00:02:52.607 --> 00:02:57.902
So far, this model is the same for
frequentists and Bayesians.

37
00:02:57.902 --> 00:03:00.783
As you may recall from
the previous course.

38
00:03:00.783 --> 00:03:04.787
The frequentist approach to
fitting this model right here.

39
00:03:04.787 --> 00:03:08.476
Would be to consider mu and
sigma to be fixed but

40
00:03:08.476 --> 00:03:12.840
unknown constants, and
then we would estimate them.

41
00:03:12.840 --> 00:03:16.106
To calculate our uncertainty
in those estimates.

42
00:03:16.106 --> 00:03:21.072
A frequentist approach would consider
how much the estimates of mu and

43
00:03:21.072 --> 00:03:22.594
sigma might change.

44
00:03:22.594 --> 00:03:26.220
If we were to repeat
the sampling process and

45
00:03:26.220 --> 00:03:30.636
obtain another sample of 15 men,
over, and over.

46
00:03:30.636 --> 00:03:34.285
The Bayesian approach,
the one we're going to take in this class.

47
00:03:34.285 --> 00:03:40.392
Tackles our uncertainty in mu and
sigma squared with probability directly.

48
00:03:40.392 --> 00:03:45.020
By treating them as random variables with
their own probability distributions.

49
00:03:46.220 --> 00:03:51.538
These are often called priors, and
they complete a Bayesian model.

50
00:03:51.538 --> 00:03:53.090
In the rest of this segment,

51
00:03:53.090 --> 00:03:56.659
we're going to review three key
components of Bayesian models.

52
00:03:56.659 --> 00:03:59.881
That were used extensively
in the previous course

53
00:04:02.642 --> 00:04:07.437
The three primary components of
Bayesian models that we often

54
00:04:07.437 --> 00:04:12.151
work with are the likelihood,
the prior and the posterior.

55
00:04:12.151 --> 00:04:13.455
The likelihood

56
00:04:19.674 --> 00:04:22.665
is the probabilistic model for the data.

57
00:04:22.665 --> 00:04:29.345
It describes how, given the unknown
parameters, the data might be generated.

58
00:04:29.345 --> 00:04:35.970
It can be written like this,
the probability of y, the theta.

59
00:04:35.970 --> 00:04:39.920
Given theta, we're going to call
unknown parameter theta right here.

60
00:04:41.080 --> 00:04:45.890
Also, in this expression, you might
recognize this from the previous class,

61
00:04:45.890 --> 00:04:49.510
as describing a probability distribution.

62
00:04:49.510 --> 00:04:55.190
It might be, for example, the density
of the distribution, if y were normal.

63
00:04:55.190 --> 00:04:59.870
Or y were discrete, this might actually
represent the probability itself.

64
00:04:59.870 --> 00:05:03.580
We're just going to use for
our notation a generic p right here.

65
00:05:05.920 --> 00:05:12.870
The prior, the next step,
is the probability

66
00:05:12.870 --> 00:05:18.510
distribution that characterizes our
uncertainty with the parameter theta.

67
00:05:18.510 --> 00:05:21.331
We're going to write it as p of theta.

68
00:05:21.331 --> 00:05:23.276
It's not the same
distribution as this one.

69
00:05:23.276 --> 00:05:27.739
We're just using this notation
p to represent the probability

70
00:05:27.739 --> 00:05:29.515
distribution of theta.

71
00:05:29.515 --> 00:05:33.034
By specifying a likelihood and a prior.

72
00:05:33.034 --> 00:05:37.381
We now have a joint probability model for
both the knowns,

73
00:05:37.381 --> 00:05:40.939
the data, and the unknowns,
the parameters.

74
00:05:40.939 --> 00:05:45.249
We can see this by using
the chain rule of probability.

75
00:05:45.249 --> 00:05:53.180
If we wanted the joint distribution of
both the data and the parameters theta.

76
00:05:53.180 --> 00:05:55.233
Using the chain rule of probability,

77
00:05:55.233 --> 00:05:57.806
we could start with
the distribution of theta.

78
00:05:57.806 --> 00:06:04.470
And multiply that by the probability or
the distribution of y given theta.

79
00:06:04.470 --> 00:06:06.990
That gives us an expression for
the joint distribution.

80
00:06:09.370 --> 00:06:13.530
However if we're going to make
inferences about data and

81
00:06:13.530 --> 00:06:16.020
we already know the values of y.

82
00:06:16.020 --> 00:06:20.407
We don't need the joint distribution, what
we need is the posterior distribution.

83
00:06:24.730 --> 00:06:29.918
The posterior distribution
is the distribution

84
00:06:29.918 --> 00:06:34.480
of theta conditional on y, theta given y.

85
00:06:35.680 --> 00:06:40.063
We can obtain this expression
right here using the laws of

86
00:06:40.063 --> 00:06:45.376
conditional probability and
specifically using Bayes' theorem.

87
00:06:45.376 --> 00:06:48.571
If we start with the laws
of conditional probability.

88
00:06:48.571 --> 00:06:53.457
The conditional distribution of
theta given y will be the joint

89
00:06:53.457 --> 00:06:55.771
distribution of theta and y.

90
00:06:55.771 --> 00:07:03.405
The same as this one right here divided by
the marginal distribution of y by itself.

91
00:07:03.405 --> 00:07:06.745
How do we get the marginal
distribution of y?

92
00:07:06.745 --> 00:07:10.391
We start with the joint
distribution like we have on top,

93
00:07:13.809 --> 00:07:19.903
And now, we're going to integrate out or
marginalize over the values of theta.

94
00:07:23.441 --> 00:07:27.696
Finally, to make this look like the Bayes
theorem that we're familiar with.

95
00:07:27.696 --> 00:07:32.580
You'll notice that the joint distribution
can be written as the product of

96
00:07:32.580 --> 00:07:34.606
the prior and the likelihood.

97
00:07:36.627 --> 00:07:38.330
I'm going to start with the likelihood.

98
00:07:39.610 --> 00:07:41.424
because that's how we
usually write Bayes' theorem.

99
00:07:45.329 --> 00:07:48.687
Again we have the same thing
in the denominator here.

100
00:07:53.559 --> 00:07:57.814
But we're going to integrate
over the values of theta.

101
00:08:01.031 --> 00:08:06.021
[COUGH] These integrals right here
are replaced by summations if

102
00:08:06.021 --> 00:08:09.983
we know that theta is
a discrete random variable.

103
00:08:09.983 --> 00:08:14.445
The marginal distribution right
here of y in the denominator is

104
00:08:14.445 --> 00:08:16.390
another important piece.

105
00:08:16.390 --> 00:08:22.122
Which you may use if you do more
advanced Bayesian modeling.

106
00:08:22.122 --> 00:08:25.667
The posterior distribution
is our primary tool for

107
00:08:25.667 --> 00:08:30.293
achieving the statistical modeling
objectives from lesson one.

108
00:08:30.293 --> 00:08:36.249
[MUSIC]