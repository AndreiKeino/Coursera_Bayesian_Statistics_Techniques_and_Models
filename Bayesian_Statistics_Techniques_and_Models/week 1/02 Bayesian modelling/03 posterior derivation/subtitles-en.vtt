WEBVTT

1
00:00:00.000 --> 00:00:03.164
[MUSIC]

2
00:00:03.164 --> 00:00:08.130
So far, we've only drawn
the model with two levels.

3
00:00:08.130 --> 00:00:12.540
But in reality, there's nothing that'll
stop us from adding more layers.

4
00:00:12.540 --> 00:00:17.160
For example, instead of fixing
the values for the hyper parameters in

5
00:00:17.160 --> 00:00:20.870
the previous segment,
those hyper parameters were the mu naught,

6
00:00:20.870 --> 00:00:23.500
the sigma naught,
the nu knot and the beta knot.

7
00:00:24.540 --> 00:00:29.570
We could specify just numbers for
those, or we could have specified

8
00:00:29.570 --> 00:00:34.270
prior distributions for those variables
to make this a hierarchical model.

9
00:00:34.270 --> 00:00:39.770
One reason we might do this is if
the data are hierarchically organized so

10
00:00:39.770 --> 00:00:42.490
that the observations
are naturally grouped together.

11
00:00:43.790 --> 00:00:47.570
We will examine these types of
hierarchical models in depth

12
00:00:47.570 --> 00:00:48.760
later in the course.

13
00:00:50.030 --> 00:00:53.160
Another simple example
of a hierarchical model

14
00:00:53.160 --> 00:00:56.870
is one you saw already
in the previous course.

15
00:00:56.870 --> 00:01:02.830
Let's write it as yi given mu and
sigma squared,

16
00:01:02.830 --> 00:01:09.830
so this is just like the model from the
previous lesson, will be independent and

17
00:01:09.830 --> 00:01:15.890
identically distributed normal with
a mean mu and a variance, sigma squared.

18
00:01:17.560 --> 00:01:21.850
The next step, instead of doing
independent priors for mu and sigma

19
00:01:21.850 --> 00:01:27.680
squared, we're going to have the prior for
mu depend on the value of sigma squared.

20
00:01:28.760 --> 00:01:34.210
That is given sigma squared,
mu follows a normal distribution

21
00:01:34.210 --> 00:01:38.360
with mean mu naught, just some hyper
parameter that you're going to chose.

22
00:01:39.620 --> 00:01:43.040
And the variance of this
prior will be sigma squared,

23
00:01:43.040 --> 00:01:47.690
this parameter, divided by omega naught.

24
00:01:47.690 --> 00:01:49.640
Another hyper parameter
that will scale it.

25
00:01:51.690 --> 00:01:57.670
We now have a joint distribution of y and
mu given sigma squared.

26
00:01:57.670 --> 00:02:02.060
So finally, we need to complete the model
with the prior 4 sigma squared.

27
00:02:03.250 --> 00:02:08.970
We'll use our standard inverse gamma with
the same hyper parameters as last time.

28
00:02:11.020 --> 00:02:12.930
This model has three layers.

29
00:02:14.370 --> 00:02:16.950
And mu depends on sigma right here.

30
00:02:18.070 --> 00:02:21.600
The graphical representation for
this model looks like this.

31
00:02:23.260 --> 00:02:26.840
We start with the variables that
don't depend on anything else.

32
00:02:26.840 --> 00:02:32.480
So that would be sigma squared and
move down the chain.

33
00:02:32.480 --> 00:02:36.700
So here, the next variable is mu
which depends on sigma squared.

34
00:02:38.910 --> 00:02:41.350
And then dependent on both,
we have the yi's.

35
00:02:44.090 --> 00:02:47.570
We use a double circle because
the yi's are observed,

36
00:02:47.570 --> 00:02:51.780
their data, and we're going to
assume that they're exchangeable.

37
00:02:51.780 --> 00:02:56.630
So let's put them on a plate here for
i in 1 to n

38
00:03:02.257 --> 00:03:06.634
The distribution of yi depends
on both mu and sigma squared, so

39
00:03:06.634 --> 00:03:10.280
we'll draw curves connecting
those pieces there.

40
00:03:11.790 --> 00:03:15.840
To simulate hypothetical data
from this model, we would

41
00:03:15.840 --> 00:03:20.160
have to first draw from the distribution
of the prior for sigma squared.

42
00:03:21.280 --> 00:03:25.530
Then the distribution for
mu which depends on sigma squared.

43
00:03:25.530 --> 00:03:30.680
And once we've drawn both of these,
then we can draw random draws from

44
00:03:30.680 --> 00:03:34.499
the y's,
which of course depends on both of those.

45
00:03:36.170 --> 00:03:40.250
With multiple levels, this is
an example of a hierarchical model.

46
00:03:43.278 --> 00:03:48.017
Once we have a model specification,
we can write out what the full

47
00:03:48.017 --> 00:03:53.459
posterior distribution for all
the parameters given the data looks like.

48
00:03:54.710 --> 00:03:58.320
Remember that the numerator
in Bayes' theorem

49
00:03:58.320 --> 00:04:03.000
is the joint distribution of all
random quantities, all the nodes

50
00:04:03.000 --> 00:04:08.570
in this graphical representation
over here from all of the layers.

51
00:04:08.570 --> 00:04:11.342
So for this model that we have right here,

52
00:04:11.342 --> 00:04:14.999
we have a joint distribution
that'll look like this.

53
00:04:16.480 --> 00:04:20.500
We're going to write the joint
distribution of everything y1 up to yn,

54
00:04:24.320 --> 00:04:27.230
mu and sigma squared,

55
00:04:29.820 --> 00:04:35.226
Using the chain rule of probability,
we're going to multiply

56
00:04:35.226 --> 00:04:40.710
all of the distributions
in the hierarchy together.

57
00:04:40.710 --> 00:04:42.350
So let's start with the likelihood piece.

58
00:04:50.220 --> 00:04:54.068
And we'll multiply that by the next layer,

59
00:04:54.068 --> 00:04:58.234
the distribution of mu,
given sigma squared.

60
00:04:58.234 --> 00:05:00.810
And finally, with the prior for
sigma squared.

61
00:05:03.296 --> 00:05:05.670
So what do these expressions
right here look like?

62
00:05:07.563 --> 00:05:12.023
The likelihood right here in
this level because they're all

63
00:05:12.023 --> 00:05:15.880
independent will be a product
of normal densities.

64
00:05:17.820 --> 00:05:22.960
So we're going to multiply
the normal density for

65
00:05:22.960 --> 00:05:30.990
each yi, Given those parameters.

66
00:05:30.990 --> 00:05:35.930
This, again, is shorthand right here for
the density of a normal distribution.

67
00:05:37.380 --> 00:05:39.170
So that represents this piece right here.

68
00:05:40.470 --> 00:05:46.180
The conditional prior of mu given
sigma squared is also a normal.

69
00:05:46.180 --> 00:05:51.050
So we're going to multiply this
by a normal distribution of mu,

70
00:05:52.190 --> 00:05:56.270
where its parameters are mu naught and
sigma squared over omega naught.

71
00:05:58.496 --> 00:06:02.570
And finally, we have the prior for
sigma squared.

72
00:06:02.570 --> 00:06:06.870
We'll multiply by the density
of an inverse gamma for

73
00:06:06.870 --> 00:06:12.470
sigma squared given the hyper
parameters mu naught,

74
00:06:12.470 --> 00:06:17.074
sorry, that is given, the hyper
parameters mu naught and and beta naught.

75
00:06:19.370 --> 00:06:24.010
What we have right here is the joint
distribution of everything.

76
00:06:24.010 --> 00:06:26.990
It is the numerator in Bayes theorem.

77
00:06:26.990 --> 00:06:31.120
Let's remind ourselves really fast
what Bayes theorem looks like again.

78
00:06:32.788 --> 00:06:37.450
We have that the posterior distribution
of the parameter given the data

79
00:06:38.990 --> 00:06:44.830
is equal to the likelihood,
Times the prior.

80
00:06:47.451 --> 00:06:49.060
Over the same thing again.

81
00:06:58.939 --> 00:07:03.149
So this gives us in the numerator the
joint distribution of everything which is

82
00:07:03.149 --> 00:07:04.950
what we've written right here.

83
00:07:07.430 --> 00:07:13.490
In Bayes theorem, the numerator and the
denominator are the exact same expression

84
00:07:13.490 --> 00:07:19.150
accept that we integrate or
marginalize over all of the parameters.

85
00:07:19.150 --> 00:07:24.660
Because the denominator is
a function of the y's only, which

86
00:07:24.660 --> 00:07:30.060
are known values,
the denominator is just a constant number.

87
00:07:30.060 --> 00:07:35.350
So we can actually write the posterior
distribution as being proportional to,

88
00:07:35.350 --> 00:07:38.410
this symbol right here
represents proportional to.

89
00:07:40.230 --> 00:07:45.070
The joint distribution of the data and
parameters, or

90
00:07:45.070 --> 00:07:50.550
the likelihood times the prior.

91
00:07:50.550 --> 00:07:56.010
The poster distribution is proportional
to the joint distribution,

92
00:07:56.010 --> 00:07:57.530
or everything we have right here.

93
00:07:59.350 --> 00:08:03.062
In other words,
what we have already written for

94
00:08:03.062 --> 00:08:09.038
this particular model is proportional
to the posterior distribution of mu and

95
00:08:09.038 --> 00:08:12.039
sigma squared, given all of the data.

96
00:08:17.336 --> 00:08:21.834
The only thing missing in this
expression right here is just some

97
00:08:21.834 --> 00:08:26.345
constant number that causes
the expression to integrate to 1.

98
00:08:27.370 --> 00:08:29.850
If we can recognize this expression

99
00:08:29.850 --> 00:08:34.430
as being proportional to a common
distribution, then our work is done,

100
00:08:34.430 --> 00:08:37.050
and we know what our
posterior distribution is.

101
00:08:38.510 --> 00:08:42.870
This was the case for
all models in the previous course.

102
00:08:42.870 --> 00:08:46.290
However, if we do not
use conjugate priors or

103
00:08:46.290 --> 00:08:50.530
if the models are more complicated,
then the posterior distribution will

104
00:08:50.530 --> 00:08:53.070
not have a standard form
that we can recognize.

105
00:08:54.290 --> 00:08:57.865
We're going to explore a couple of

106
00:08:57.865 --> 00:09:02.436
examples of this issue
in the next segment.

107
00:09:02.436 --> 00:09:05.479
[MUSIC]