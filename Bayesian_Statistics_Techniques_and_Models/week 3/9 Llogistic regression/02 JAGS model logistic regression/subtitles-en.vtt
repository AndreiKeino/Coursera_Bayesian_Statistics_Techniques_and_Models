WEBVTT

1
00:00:00.000 --> 00:00:03.399
For an example of logistic regression,

2
00:00:03.399 --> 00:00:08.769
we're going to use the urine data set from the boot package in R.

3
00:00:08.769 --> 00:00:15.960
First, we'll need to load the boot package.

4
00:00:15.960 --> 00:00:20.199
And if it is not already installed, you'll have to do that as well.

5
00:00:20.199 --> 00:00:32.859
After loading the package, we can load the data which is called "urine".

6
00:00:32.859 --> 00:00:40.289
This data contain seven characteristics of 79 urine specimens.

7
00:00:40.289 --> 00:00:43.645
The goal of the analysis is to determine

8
00:00:43.645 --> 00:00:49.149
if certain physical characteristics of the urine are related to the formation

9
00:00:49.149 --> 00:00:52.404
of calcium oxalate crystals.

10
00:00:52.404 --> 00:00:57.984
Our seven variables are, first, R, which is an indicator

11
00:00:57.984 --> 00:01:05.165
of the presence of a calcium oxalate crystal, so the value of R will be zero or one,

12
00:01:05.165 --> 00:01:11.140
and our explanatory variables or covariates are the specific gravity,

13
00:01:11.140 --> 00:01:15.510
the pH reading (this is for acidity),

14
00:01:15.510 --> 00:01:19.750
the osmolarity, conductivity,

15
00:01:19.750 --> 00:01:26.239
the urea concentration, and finally, the calcium concentration.

16
00:01:26.239 --> 00:01:29.290
For more information about this data,

17
00:01:29.290 --> 00:01:32.234
see the following source.

18
00:01:32.234 --> 00:01:35.689
Let's see what the data looks like.

19
00:01:35.689 --> 00:01:41.965
We'll take the head of the urine data set.

20
00:01:41.965 --> 00:01:50.994
We can see that calcium oxalate crystals were not observed in the first six specimens,

21
00:01:50.994 --> 00:01:57.254
and we have values for the six other explanatory variables.

22
00:01:57.254 --> 00:02:00.680
Notice that we have missing values in this data set.

23
00:02:00.680 --> 00:02:12.039
So, before we conduct analysis, let's remove those missing values.

24
00:02:12.039 --> 00:02:17.715
'dat' is the data set that we're going to use.

25
00:02:17.715 --> 00:02:21.810
We went from 79 rows down to 77 rows.

26
00:02:21.810 --> 00:02:25.530
Now, let's look at a pairwise scatterplot for each

27
00:02:25.530 --> 00:02:35.775
of the seven variables using pairs on our data set.

28
00:02:35.775 --> 00:02:37.740
One thing that stands out

29
00:02:37.740 --> 00:02:42.965
is that several of these variables are strongly correlated with one another.

30
00:02:42.965 --> 00:02:48.840
For example, the specific gravity and the osmolarity

31
00:02:48.840 --> 00:02:54.925
appear to have a very close linear relationship, as you can see in this scatterplot.

32
00:02:54.925 --> 00:03:02.594
Collinearity between x variables or explanatory variables in linear regression models

33
00:03:02.594 --> 00:03:05.694
can cause trouble for statistical inference.

34
00:03:05.694 --> 00:03:12.360
Two correlated variables will compete for the ability to predict the response variable,

35
00:03:12.360 --> 00:03:15.840
leading to unstable estimation.

36
00:03:15.840 --> 00:03:19.504
This is not a problem for prediction of the response.

37
00:03:19.504 --> 00:03:25.145
So, if prediction is our only goal of the analysis, then it's not a problem.

38
00:03:25.145 --> 00:03:30.659
But, if our objective is to discover how the variables relate to the response,

39
00:03:30.659 --> 00:03:34.329
we should avoid collinearity.

40
00:03:34.329 --> 00:03:38.659
Because the primary goal of this analysis

41
00:03:38.659 --> 00:03:42.830
is to determine which variables are related to the presence

42
00:03:42.830 --> 00:03:45.560
of calcium oxalate crystals,

43
00:03:45.560 --> 00:03:50.719
we will have to deal with the collinearity between the predictors.

44
00:03:50.719 --> 00:03:54.814
This problem of determining which variables relate to the response

45
00:03:54.814 --> 00:03:57.930
is often called variable selection.

46
00:03:57.930 --> 00:04:00.629
We have already seen one way to do this.

47
00:04:00.629 --> 00:04:05.344
We could fit several models that include different sets of variables

48
00:04:05.344 --> 00:04:11.939
and see which one of those models has the best deviance information criterion value.

49
00:04:11.939 --> 00:04:13.460
Another way to do this

50
00:04:13.460 --> 00:04:18.410
is to use a linear model where the priors for the linear coefficients

51
00:04:18.410 --> 00:04:21.545
favors values near zero.

52
00:04:21.545 --> 00:04:27.569
Values near zero on the coefficients indicate weak relationships.

53
00:04:27.569 --> 00:04:30.350
This way, the burden of establishing

54
00:04:30.350 --> 00:04:34.685
an association between the variables lies with the data.

55
00:04:34.685 --> 00:04:39.139
If there is not a strong signal, we assume that it does not exist

56
00:04:39.139 --> 00:04:42.709
because the prior favors that scenario.

57
00:04:42.709 --> 00:04:48.845
Rather than tailoring a specific prior for each individual beta coefficient

58
00:04:48.845 --> 00:04:54.454
based on the scale of its covariate X, it is customary

59
00:04:54.454 --> 00:05:00.805
to subtract the mean and divide by the standard deviation for each variable.

60
00:05:00.805 --> 00:05:02.550
It's easy to do this in R.

61
00:05:02.550 --> 00:05:10.379
We can create the design matrix X by scaling the data.

62
00:05:10.379 --> 00:05:13.305
We want to omit the first column

63
00:05:13.305 --> 00:05:19.620
because the first column contains our response variable, so we want the next six columns.

64
00:05:19.620 --> 00:05:25.649
We want to scale those by centering them, so center=true,

65
00:05:25.649 --> 00:05:33.019
and we also want to scale them, dividing by the standard deviation.

66
00:05:33.019 --> 00:05:38.834
This only makes sense to do for variables that are continuous.

67
00:05:38.834 --> 00:05:43.415
If we have categorical variables, we should exclude them from this

68
00:05:43.415 --> 00:05:46.704
centering and scaling exercise.

69
00:05:46.704 --> 00:05:49.745
Let's run the X.

70
00:05:49.745 --> 00:05:51.944
Oops, I called it 'data' instead of 'dat'.

71
00:05:51.944 --> 00:05:53.735
Let's rerun that,

72
00:05:53.735 --> 00:05:59.139
and take a look at the first several values.

73
00:05:59.990 --> 00:06:07.959
We can see that the values for these x variables have changed, so that

74
00:06:07.959 --> 00:06:12.839
the column means for X are all close to zero.

75
00:06:12.839 --> 00:06:17.754
For example, -9.8*10^(-15).

76
00:06:17.754 --> 00:06:19.836
That's very close to zero,

77
00:06:19.836 --> 00:06:25.060
and that is the case with all of these column means.

78
00:06:25.060 --> 00:06:30.555
We can also calculate the column standard deviations using the apply function.

79
00:06:30.555 --> 00:06:40.439
Apply, where we want to apply this function, standard deviation

80
00:06:40.439 --> 00:06:46.105
to the columns which is the second index of the dimensions of X.

81
00:06:46.105 --> 00:06:50.329
The rows are index one and the columns are index two.

82
00:06:50.329 --> 00:06:58.055
So, let's calculate the column standard deviations of X, and they're all one.

83
00:06:58.055 --> 00:07:01.894
Now that the explanatory variables or

84
00:07:01.894 --> 00:07:06.514
covariates are centered and scaled, let's talk about that special prior

85
00:07:06.514 --> 00:07:11.485
which will favor coefficients near zero.

86
00:07:11.485 --> 00:07:13.485
The prior we're going to use

87
00:07:13.485 --> 00:07:19.415
is the double exponential, also referred to, as the Laplace prior.

88
00:07:19.415 --> 00:07:24.290
Here we have the standard normal PDF, as a dashed line,

89
00:07:24.290 --> 00:07:29.787
and the PDF of the double exponential distribution, as a solid line.

90
00:07:29.787 --> 00:07:31.660
As the name implies,

91
00:07:31.660 --> 00:07:36.894
the double exponential distribution looks like an exponential distribution

92
00:07:36.894 --> 00:07:40.060
with tails extending in the positive direction

93
00:07:40.060 --> 00:07:42.639
as well as in the negative direction,

94
00:07:42.639 --> 00:07:46.449
whereas the exponential only went in the positive direction.

95
00:07:46.449 --> 00:07:51.105
We also have a sharp peak at zero.

96
00:07:51.105 --> 00:07:56.745
We can read more about the double exponential distribution in the JAGS documentation

97
00:07:56.745 --> 00:08:04.759
or the JAGS manual under 'distributions'.

98
00:08:04.759 --> 00:08:08.694
Next, let's set up and talk about this model in JAGS.

99
00:08:08.694 --> 00:08:11.509
Here is the model string.

100
00:08:11.509 --> 00:08:15.920
It looks pretty similar to the linear regressions we used earlier,

101
00:08:15.920 --> 00:08:19.384
except with some important modifications.

102
00:08:19.384 --> 00:08:27.379
In the likelihood portion of the model, we can see that y_i, the ith observation,

103
00:08:27.379 --> 00:08:33.064
now comes from a Bernoulli distribution 'dbern' rather than a 'dnorm',

104
00:08:33.064 --> 00:08:36.394
where the probability of success for

105
00:08:36.394 --> 00:08:43.985
observing a one for the ith observation is probability sub i or p_i.

106
00:08:43.985 --> 00:08:47.575
Instead of modeling p_i directly,

107
00:08:47.575 --> 00:08:52.575
in logistic regression, we're going to model the logit of p.

108
00:08:52.575 --> 00:08:57.105
The logit of p gets the linear model portion

109
00:08:57.105 --> 00:09:03.179
where we have a parameter for the intercept, and now we have a beta coefficient

110
00:09:03.179 --> 00:09:06.809
for each of the six individual variables.

111
00:09:06.809 --> 00:09:11.934
We'll place a fairly non-informative prior on the intercept.

112
00:09:11.934 --> 00:09:17.860
It'll be a normal prior with variance 25 or standard deviation 5.

113
00:09:17.860 --> 00:09:21.835
That's fairly non-informative in a logistic regression model

114
00:09:21.835 --> 00:09:28.269
where values as high as two or three can get you very close to probability one

115
00:09:28.269 --> 00:09:34.990
and values of negative two or three can get you very close to probability zero.

116
00:09:34.990 --> 00:09:40.774
Next, we will create our prior for the individual beta terms.

117
00:09:40.774 --> 00:09:47.074
Beta j will get a 'ddexp' or a double exponential prior

118
00:09:47.074 --> 00:09:53.870
with location parameter zero, and this value for the inverse scale parameter.

119
00:09:53.870 --> 00:09:56.960
As we've noted, this value for the inverse scale

120
00:09:56.960 --> 00:10:01.924
gives us a double exponential distribution with variance one.

121
00:10:01.924 --> 00:10:04.970
The rest of these lines should look familiar to you.

122
00:10:04.970 --> 00:10:08.909
So, let's go ahead and run them.

123
00:10:11.990 --> 00:10:17.664
We won't do all of the convergence diagnostics right now, but

124
00:10:17.664 --> 00:10:20.884
let's look at least at the trace plots.

125
00:10:20.884 --> 00:10:29.340
Let's run that.

126
00:10:29.340 --> 00:10:34.004
These trace plots look like they might have some high autocorrelation,

127
00:10:34.004 --> 00:10:36.610
but they don't look too bad.

128
00:10:36.610 --> 00:10:40.404
We'll have to click 'return' to get the next plots

129
00:10:40.404 --> 00:10:44.504
for the remaining betas and for the intercept.

130
00:10:44.504 --> 00:10:47.434
These trace plots look OK.

131
00:10:47.434 --> 00:10:53.144
Before we move on, let's also calculate the DIC for this model.

132
00:10:53.144 --> 00:10:57.854
Instead of a numerical summary of our posterior distribution,

133
00:10:57.854 --> 00:11:01.429
let's take a look at a visual summary.

134
00:11:01.429 --> 00:11:08.239
Let's create the marginal posterior density plots for our six beta coefficients.

135
00:11:08.239 --> 00:11:14.230
To display these all on the same page, we'll use par(mfrow),

136
00:11:14.230 --> 00:11:20.684
where we want three rows and two columns in our plots.

137
00:11:20.684 --> 00:11:24.904
Now we'll use the denseplot function

138
00:11:24.904 --> 00:11:35.029
on our combined simulations mod1csim and we want the first six columns

139
00:11:35.029 --> 00:11:39.110
which contain the six beta variables.

140
00:11:39.110 --> 00:11:41.659
We also want these to be displayed

141
00:11:41.659 --> 00:11:47.419
on an identical x-axis, so let's give it limits for that x-axis.

142
00:11:47.419 --> 00:11:53.330
Xlim will go from negative three to positive three.

143
00:11:53.330 --> 00:11:57.729
To remind ourselves, which beta goes with which variable,

144
00:11:57.729 --> 00:12:03.434
let's take the column names from X.

145
00:12:03.434 --> 00:12:13.014
So, beta1 is the specific gravity, beta2 is the pH, acidity, and so forth.

146
00:12:13.014 --> 00:12:20.889
Let's generate these plots.

147
00:12:20.889 --> 00:12:23.629
The first thing we're looking for in these plots

148
00:12:23.629 --> 00:12:31.855
is evidence that the different coefficients, the betas, are different from zero.

149
00:12:31.855 --> 00:12:39.040
It is clear that the coefficients for gravity, which is beta1, conductivity,

150
00:12:39.040 --> 00:12:41.965
which is beta4 here,

151
00:12:41.965 --> 00:12:47.004
and calcium concentration, which is beta6 here,

152
00:12:47.004 --> 00:12:52.504
all three of these betas are significantly away from zero.

153
00:12:52.504 --> 00:12:57.355
Notice that the coefficient for osmolarity, the beta3,

154
00:12:57.355 --> 00:13:00.700
the posterior distribution here looks like

155
00:13:00.700 --> 00:13:06.789
our double exponential prior, and it is almost centered on zero, still.

156
00:13:06.789 --> 00:13:09.460
So, we're going to conclude that osmolarity

157
00:13:09.460 --> 00:13:14.225
is not a strong predictor of calcium oxalate crystals.

158
00:13:14.225 --> 00:13:17.574
The same goes for pH up here.

159
00:13:17.574 --> 00:13:26.049
Urea concentration, or beta5 over here, looks like it's a borderline case.

160
00:13:26.049 --> 00:13:30.865
However, if we refer back to our pairwise scatter plots,

161
00:13:30.865 --> 00:13:37.130
we see that urea concentration is highly correlated with specific gravity,

162
00:13:37.130 --> 00:13:41.034
and so we're going to remove this variable.

163
00:13:41.034 --> 00:13:48.595
That is, we're going to create another model where we keep the coefficients for gravity,

164
00:13:48.595 --> 00:13:58.970
conductivity and calcium concentration only.

165
00:13:58.970 --> 00:14:02.840
Here is that model string for the next model.

166
00:14:02.840 --> 00:14:06.919
We've removed the variables that were not significantly away from zero

167
00:14:06.919 --> 00:14:12.259
and retained the coefficient for gravity, a coefficient for conductivity

168
00:14:12.259 --> 00:14:14.965
and the coefficient for calcium.

169
00:14:14.965 --> 00:14:19.649
We've also switched from the double exponential prior

170
00:14:19.649 --> 00:14:25.024
to a fairly non-informative normal prior on these coefficients.

171
00:14:25.024 --> 00:14:29.394
Let's now run this model.

172
00:14:29.394 --> 00:14:34.945
We get a warning message: The JAGS did not use the three variables pH,

173
00:14:34.945 --> 00:14:38.830
osmolarity and urea (which is not surprising to us).

174
00:14:38.830 --> 00:14:41.600
It won't affect our analysis.

175
00:14:41.600 --> 00:14:46.335
I'll skip ahead and run the remaining code for this model.

176
00:14:46.335 --> 00:14:50.170
We need to run the burn-in period,

177
00:14:50.170 --> 00:14:59.595
the posterior samples and the DIC calculation for this model.

178
00:14:59.595 --> 00:15:05.389
I'll leave it up to you to check the convergence diagnostics here.

179
00:15:06.950 --> 00:15:15.174
Now that we have DIC for both models one and two, why don't we compare the two,

180
00:15:15.174 --> 00:15:23.429
or run 'dic1' and 'dic2'.

181
00:15:23.429 --> 00:15:25.919
And we can see immediately that the first model has

182
00:15:25.919 --> 00:15:29.909
more of a penalty, because there are more parameters in that model.

183
00:15:29.909 --> 00:15:31.830
And interestingly,

184
00:15:31.830 --> 00:15:38.294
the first model has the lower DIC of the two, although they're pretty close.

185
00:15:38.294 --> 00:15:40.605
It's also important to note

186
00:15:40.605 --> 00:15:44.804
that we did change the prior between the two models,

187
00:15:44.804 --> 00:15:50.404
and generally we should not use DIC to choose between priors.

188
00:15:50.404 --> 00:15:55.205
So, in this case, we may not be making a fair comparison.

189
00:15:55.205 --> 00:16:02.139
Nevertheless, let's check our results from the second model.

190
00:16:02.519 --> 00:16:08.884
Oops, that was a summary of the model object itself.

191
00:16:08.884 --> 00:16:14.735
What we want is a summary of the model simulations.

192
00:16:14.735 --> 00:16:16.644
There we go.

193
00:16:16.644 --> 00:16:21.879
So, we can see that with a positive coefficient, the specific gravity

194
00:16:21.879 --> 00:16:30.299
is associated with an increase in the probability of observing crystals.

195
00:16:30.299 --> 00:16:34.004
With a negative coefficient, the conductivity

196
00:16:34.004 --> 00:16:40.455
is associated with a decrease in the probability of observing oxalate crystals,

197
00:16:40.455 --> 00:16:43.669
and with a positive coefficient here,

198
00:16:43.669 --> 00:16:49.424
we conclude that calcium concentration is associated with an increase

199
00:16:49.424 --> 00:16:53.632
in probability of calcium oxalate crystals.

200
00:16:53.632 --> 00:16:59.559
Each of these estimates are several posterior standard deviations

201
00:16:59.559 --> 00:17:04.255
away from zero, so we would say they are significant contributors

202
00:17:04.255 --> 00:17:08.380
to the probability of calcium oxalate crystals.

203
00:17:08.380 --> 00:17:12.199
Are these the same conclusions we reached from the first model?

204
00:17:12.199 --> 00:17:21.160
Let's look at the combined simulation and just look at the point estimates.

205
00:17:21.160 --> 00:17:30.140
Remember, specific gravity was the first coefficient: 1.63, pretty similar here.

206
00:17:30.140 --> 00:17:36.384
The fourth coefficient was conductivity, had a negative effect

207
00:17:36.384 --> 00:17:44.180
which we see in both models, and finally, calcium concentration was the sixth variable.

208
00:17:44.180 --> 00:17:46.200
It was estimated at 1.62.

209
00:17:46.200 --> 00:17:49.180
Here, it was estimated at 1.88.

210
00:17:49.180 --> 00:17:51.284
Although the numbers are a little different,

211
00:17:51.284 --> 00:17:55.454
the two models essentially yield the same conclusions.

212
00:17:55.454 --> 00:17:59.984
There are more modeling options available in this scenario,

213
00:17:59.984 --> 00:18:03.375
perhaps including transformations of variables,

214
00:18:03.375 --> 00:18:07.787
different priors or interactions between the predictors,

215
00:18:07.787 --> 00:18:12.000
but we'll leave it to you to see if you can improve this model.