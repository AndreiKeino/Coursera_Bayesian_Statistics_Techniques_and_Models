WEBVTT

1
00:00:00.000 --> 00:00:03.099
[MUSIC]

2
00:00:03.099 --> 00:00:06.536
We discussed linear regression
briefly in the previous course.

3
00:00:06.536 --> 00:00:10.289
And we fit a few models with
non-informative priors.

4
00:00:10.289 --> 00:00:13.474
Here, we'll provide a brief review,

5
00:00:13.474 --> 00:00:18.114
demonstrate fitting linear
regression models in jags.

6
00:00:18.114 --> 00:00:22.860
And discuss a few practical skills that
are helpful when fitting linear models in

7
00:00:22.860 --> 00:00:23.500
general.

8
00:00:24.560 --> 00:00:28.460
This is not meant to be a comprehensive
treatment of linear models,

9
00:00:28.460 --> 00:00:31.180
which you can find in numerous courses and
textbooks.

10
00:00:32.970 --> 00:00:36.730
Linear regression is perhaps
the simplest way to relate

11
00:00:36.730 --> 00:00:41.000
a continuous response variable to
multiple explanatory variables.

12
00:00:42.240 --> 00:00:45.799
This may arise from observing
several variables together and

13
00:00:45.799 --> 00:00:49.985
investigating which variables
correlate with the response variable.

14
00:00:49.985 --> 00:00:54.706
Or it could arise from conducting
an experiment, where we carefully assign

15
00:00:54.706 --> 00:00:58.852
values of explanatory variables
to randomly selected subjects.

16
00:00:58.852 --> 00:01:03.251
And try to establish a cause and
effect relationship.

17
00:01:03.251 --> 00:01:10.468
A linear regression model has
the following form,the response y for

18
00:01:10.468 --> 00:01:18.860
observation i,will be equal to this
linear form of the other variable.

19
00:01:18.860 --> 00:01:21.610
We have an intercept, beta not, and

20
00:01:21.610 --> 00:01:26.710
then a coefficient beta 1 for
the first x variable.

21
00:01:26.710 --> 00:01:32.270
And so forth for up to k variables.

22
00:01:32.270 --> 00:01:35.140
So this would be the ith value of xk.

23
00:01:36.960 --> 00:01:41.950
This describes the mean, and
then we would also add an error,

24
00:01:41.950 --> 00:01:45.230
individual term for observation I.

25
00:01:47.600 --> 00:01:53.465
We would assume that the Epsilons
are IID from a normal distribution means

26
00:01:53.465 --> 00:02:00.200
0 variance sigma squared for
observations 1 up to n.

27
00:02:01.470 --> 00:02:07.700
Equivalently we can write this model for
y, i directly as y,

28
00:02:07.700 --> 00:02:13.600
i given all of the x values for
individual i.

29
00:02:15.530 --> 00:02:17.650
All of the Beta coefficients and

30
00:02:17.650 --> 00:02:23.450
sigma squared would be
independently distributed,

31
00:02:23.450 --> 00:02:27.770
not identically distributed, because
the mean changes for each observation.

32
00:02:27.770 --> 00:02:31.584
But independent from
a normal with the same mean.

33
00:02:31.584 --> 00:02:35.670
Beta 0 + beta 1 x1i up for

34
00:02:35.670 --> 00:02:39.233
all of our k variables.

35
00:02:41.891 --> 00:02:44.870
And constant variance sigma squared.

36
00:02:46.840 --> 00:02:49.530
Again, k is the number
of predictor variables.

37
00:02:50.870 --> 00:02:53.720
This yields the following
graphical model structure.

38
00:02:55.620 --> 00:03:04.151
We'll start with a plate for
our All of our different y variables,

39
00:03:04.151 --> 00:03:08.593
so yi, which is random and observed.

40
00:03:08.593 --> 00:03:13.829
Or i = 1 up to n.

41
00:03:13.829 --> 00:03:17.464
We also observe the x variables.

42
00:03:17.464 --> 00:03:22.334
So we have X1i, X2i,

43
00:03:22.334 --> 00:03:26.717
all the way up to Xki.

44
00:03:26.717 --> 00:03:31.055
And I'm going to draw squares
around these ones instead

45
00:03:31.055 --> 00:03:35.680
of circles to indicate that
they are not random variables.

46
00:03:35.680 --> 00:03:38.360
We're always conditioning on the Xs.

47
00:03:41.640 --> 00:03:42.790
So they'll just be constants.

48
00:03:44.220 --> 00:03:49.027
Of course, the yi's depend
on the values of the x's and

49
00:03:49.027 --> 00:03:54.358
the yi's will also depend on
the values of these parameters.

50
00:03:54.358 --> 00:03:59.750
So, we have beta not,
beta one, up to beta k.

51
00:04:01.420 --> 00:04:02.880
And of course, sigma squared.

52
00:04:08.116 --> 00:04:11.176
And the yi's depend on all of these.

53
00:04:11.176 --> 00:04:15.130
So this would be the graphical
model representation.

54
00:04:17.310 --> 00:04:21.200
The terms of a linear model
are always linearly related

55
00:04:21.200 --> 00:04:23.550
because of the structure of the model.

56
00:04:23.550 --> 00:04:29.300
But the model does not have to be linear
necessarily in the xy relationship.

57
00:04:29.300 --> 00:04:35.080
For example, it may be that y is related
linearly to x squared instead of x.

58
00:04:36.240 --> 00:04:40.830
Hence we could transform the x and
y variables to get new x's and

59
00:04:40.830 --> 00:04:45.380
new y's but
we would still have a linear model.

60
00:04:45.380 --> 00:04:48.190
However, in that case,
if we transform the variables,

61
00:04:48.190 --> 00:04:52.980
we must be careful about how this changes
the final interpretation of the results.

62
00:04:55.030 --> 00:04:59.470
The basic interpretation of
the beta coefficients is this.

63
00:05:00.610 --> 00:05:05.210
While holding all other x
variables constant, if x1,

64
00:05:05.210 --> 00:05:08.980
for example, increases by one,

65
00:05:08.980 --> 00:05:15.440
then the mean of y is expected
to increase by beta one.

66
00:05:15.440 --> 00:05:20.230
That is beta 1 describes
how the mean of y changes

67
00:05:20.230 --> 00:05:25.700
with changes in x1, while accounting for
all the other x variables.

68
00:05:25.700 --> 00:05:27.290
That's true for all of the x variables.

69
00:05:29.150 --> 00:05:33.670
Note that we're going to assume that
the ys are independent of each other,

70
00:05:33.670 --> 00:05:35.290
given the xs.

71
00:05:35.290 --> 00:05:37.040
And that they have the same variance.

72
00:05:38.630 --> 00:05:41.972
These are strong assumptions
that are not always met.

73
00:05:41.972 --> 00:05:45.130
And there are many statistical
models to address that.

74
00:05:46.190 --> 00:05:48.980
We'll look at some hierarchical
methods in the coming lessons.

75
00:05:51.870 --> 00:05:55.425
Of course, we need to complete
the model with prior distributions.

76
00:05:55.425 --> 00:05:59.035
So we might say beta 0
comes from its prior.

77
00:06:00.940 --> 00:06:07.964
Beta 1 would come from its prior,
and so forth for all the betas.

78
00:06:07.964 --> 00:06:11.224
And sigma squared would
come from its prior.

79
00:06:13.326 --> 00:06:19.210
The most common choice for prior on
the betas, is a normal distribution.

80
00:06:19.210 --> 00:06:22.550
Or we can do a multivariate normal for
all of the betas at once.

81
00:06:23.780 --> 00:06:28.160
This is conditionally conjugate and
allows us to do Gibbs sampling.

82
00:06:29.370 --> 00:06:31.489
If we want to be non informative,

83
00:06:31.489 --> 00:06:34.984
we can choose normal priors
with very large variance.

84
00:06:34.984 --> 00:06:39.083
Which are practically flat for
realistic values of beta.

85
00:06:39.083 --> 00:06:43.879
The non-informative priors used in
the last class are equivalent to

86
00:06:43.879 --> 00:06:47.110
using normal priors
with infinite variance.

87
00:06:48.320 --> 00:06:51.950
We can also use the conditionally
conjugate inverse gamma prior for

88
00:06:51.950 --> 00:06:53.780
sigma squared that we're familiar with.

89
00:06:55.780 --> 00:07:01.290
Another common prior for
the betas is double exponential,

90
00:07:01.290 --> 00:07:06.510
or the Laplace prior, or
Laplace distribution.

91
00:07:10.410 --> 00:07:15.450
The Laplace prior has this density.

92
00:07:16.850 --> 00:07:21.130
One half e to the negative,
absolute value of beta.

93
00:07:22.270 --> 00:07:23.780
And the density looks like this.

94
00:07:29.631 --> 00:07:34.119
It's called double exponential
because it looks like the exponential

95
00:07:34.119 --> 00:07:37.870
distribution except it's been
reflected over the y axis.

96
00:07:38.970 --> 00:07:43.938
It has a sharp peak at x equals 0,
or beta equals 0 in this case,

97
00:07:43.938 --> 00:07:49.366
which can be useful if we want to do
variable selection among our x's.

98
00:07:49.366 --> 00:07:52.180
Because it'll favor values in your 0 for
these betas.

99
00:07:53.730 --> 00:07:57.598
This is related to the popular
regression technique known as the LASSO.

100
00:07:57.598 --> 00:08:03.879
[MUSIC]