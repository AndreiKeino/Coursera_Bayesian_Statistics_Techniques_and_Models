WEBVTT

1
00:00:00.910 --> 00:00:04.830
Let's recreate this model now using JAGS.

2
00:00:04.830 --> 00:00:08.462
First, we'll need to
load the rjags library.

3
00:00:13.694 --> 00:00:19.230
And we'll specify the hierarchical form
of the model, let's call this mod1.

4
00:00:21.470 --> 00:00:22.870
There will be several different models.

5
00:00:22.870 --> 00:00:29.508
So let's number this one and we're
going to start with the model string.

6
00:00:29.508 --> 00:00:35.033
The syntax for this in JAGS is to
say model with curly braces and

7
00:00:35.033 --> 00:00:37.803
fill in the different pieces.

8
00:00:37.803 --> 00:00:44.370
Let's start with the likelihood for
i in 1:n the length of the data.

9
00:00:47.358 --> 00:00:53.167
Observation y[i] will come from

10
00:00:53.167 --> 00:00:58.380
a normal distribution dnorm.

11
00:00:58.380 --> 00:01:04.156
Where the mean is mu[i],
we'll specify what that is in a moment.

12
00:01:04.156 --> 00:01:08.630
And precision,
we'll call it P-R-E-C for now.

13
00:01:12.490 --> 00:01:16.192
Now we need to add the linear
model part of this.

14
00:01:16.192 --> 00:01:19.739
mu[i] is linear,

15
00:01:19.739 --> 00:01:25.580
where we have coefficient 1 plus

16
00:01:25.580 --> 00:01:32.257
coefficient 2 times the log income,

17
00:01:32.257 --> 00:01:37.900
the ith value of the log income.

18
00:01:38.900 --> 00:01:43.910
So, y[i] comes from this normal
distribution with mean mu[i], and

19
00:01:43.910 --> 00:01:49.360
the mean comes from our
linear model of log_income.

20
00:01:49.360 --> 00:01:55.617
We now need to give a prior for
these coefficients.

21
00:01:55.617 --> 00:02:02.115
We'll start another loop for j in 1:2,
because they're two coefficients.

22
00:02:04.005 --> 00:02:09.457
Beta j will come from
a normal distribution, dnorm

23
00:02:14.338 --> 00:02:19.842
With let's say mean 0 and
a variance 1 million.

24
00:02:19.842 --> 00:02:23.768
We'll make it pretty large so
that this prior is close to

25
00:02:23.768 --> 00:02:27.370
the non-informative prior
model we fit earlier.

26
00:02:27.370 --> 00:02:31.213
Remember, we have to enter it as
a precision for the normal distribution.

27
00:02:31.213 --> 00:02:38.044
So this will be 1 divided by the variance
of 1.0 times 10 to the 6th power.

28
00:02:42.654 --> 00:02:47.421
We'll use an inverse gamma distribution
on the variants of the normal

29
00:02:47.421 --> 00:02:50.019
distribution for the observations.

30
00:02:51.130 --> 00:02:56.040
An inverse gamma on the variance
implies a gamma prior

31
00:02:56.040 --> 00:02:59.820
on the precision which is
the reciprocal of the variance.

32
00:02:59.820 --> 00:03:02.200
So let's put the prior
directly on the precision.

33
00:03:03.550 --> 00:03:08.724
Precision will come from
a gamma distribution.

34
00:03:08.724 --> 00:03:13.595
And remember, it's very important
to check the JAGS documentation,

35
00:03:13.595 --> 00:03:17.422
the JAGS manual,
under the chapter on distributions.

36
00:03:17.422 --> 00:03:22.710
To remind ourselves how JAGS
parameterizes these distributions.

37
00:03:22.710 --> 00:03:25.650
In this case, it is the same as with R.

38
00:03:25.650 --> 00:03:29.160
The first parameter for
the gamma distribution is a shape.

39
00:03:29.160 --> 00:03:30.990
And the second parameter is a rate.

40
00:03:33.040 --> 00:03:37.680
Let's use the same trick we did before,
where we parameterized

41
00:03:37.680 --> 00:03:42.200
the prior in terms of a prior
sample size and a prior guess.

42
00:03:43.510 --> 00:03:46.064
We'll use a prior sample size of 5.

43
00:03:47.080 --> 00:03:51.610
And recall we have to divide it by 2
to turn it into the gamma parameter.

44
00:03:53.100 --> 00:03:57.883
And then we have to do
the prior sample size again

45
00:03:57.883 --> 00:04:01.950
times our prior guess for the variance,

46
00:04:01.950 --> 00:04:07.109
let's say it's 10, and
divide that also by 2.

47
00:04:10.728 --> 00:04:15.307
A gamma prior on the precision
using this shape and

48
00:04:15.307 --> 00:04:18.698
this rate parameter is equivalent.

49
00:04:18.698 --> 00:04:24.585
To an inverse gamma prior on the variance
with this shape parameter and

50
00:04:24.585 --> 00:04:27.090
this as the scale parameter.

51
00:04:31.150 --> 00:04:35.180
Instead of precision we will probably
be interested in the variance,

52
00:04:36.830 --> 00:04:40.510
which is equal to 1 over the precision.

53
00:04:42.390 --> 00:04:45.490
Notice we didn't assign
a distribution here

54
00:04:45.490 --> 00:04:48.620
because we gave a prior to the precision.

55
00:04:48.620 --> 00:04:50.420
That implies a prior for

56
00:04:50.420 --> 00:04:53.530
sigma squared, the inverse
gamma we've been talking about.

57
00:04:54.900 --> 00:04:59.404
So instead of a distribution here,
we give it an equals.

58
00:04:59.404 --> 00:05:05.743
This is a deterministic relationship
between the precision, and the variance.

59
00:05:05.743 --> 00:05:10.028
Usually when we're modeling, the most
interpretable quantity is the standard

60
00:05:10.028 --> 00:05:12.760
deviation instead of the variance.

61
00:05:12.760 --> 00:05:15.700
So let's monitor the standard deviation,

62
00:05:15.700 --> 00:05:18.930
which is just the square
root of the variance.

63
00:05:21.570 --> 00:05:24.884
This completes the model
where this section tells us

64
00:05:24.884 --> 00:05:26.550
the likelihood of the data.

65
00:05:28.420 --> 00:05:32.343
This section gives us the priors for
the coefficients, and

66
00:05:32.343 --> 00:05:37.613
down here we have the prior for the left
over, or the unaccounted for variants.

67
00:05:40.840 --> 00:05:45.650
JAGS requires that we
put this in a string.

68
00:05:45.650 --> 00:05:48.591
So I'll put these between quotation marks.

69
00:05:50.531 --> 00:05:53.913
And now we need to set up the model.

70
00:05:53.913 --> 00:05:56.761
First, let's set the random seed.

71
00:06:00.341 --> 00:06:03.264
And we'll tell it what the data are.

72
00:06:03.264 --> 00:06:08.979
So data for
model 1 that'll go into JAGS needs

73
00:06:08.979 --> 00:06:15.582
to be a list where we named
our variable y in the model.

74
00:06:15.582 --> 00:06:18.251
So we have to give it y down here.

75
00:06:18.251 --> 00:06:22.675
y=dat, we're using our modified

76
00:06:22.675 --> 00:06:27.983
dataset where we removed missing values,

77
00:06:27.983 --> 00:06:33.446
dat$loginfant as our response variable.

78
00:06:33.446 --> 00:06:38.565
n is the number of rows
in the dat dataset.

79
00:06:40.717 --> 00:06:45.187
And, we also created a variable
here called log_income.

80
00:06:45.187 --> 00:06:50.047
So we need to give it the exact
same name down here, log_income.

81
00:06:52.930 --> 00:06:58.023
We'll get dat$logincome.

82
00:06:58.023 --> 00:07:00.965
That's how we specified
it in the data set.

83
00:07:04.266 --> 00:07:09.832
The parameters that we want to monitor for

84
00:07:09.832 --> 00:07:14.762
model one are the two coefficients,

85
00:07:14.762 --> 00:07:17.792
beta one and beta two.

86
00:07:17.792 --> 00:07:24.330
And since they were specified and indexed
together as a vector I can just call it b.

87
00:07:27.511 --> 00:07:32.372
We have our choice if we want
to monitor the precision,

88
00:07:32.372 --> 00:07:38.092
the variance, or the standard
deviation from the likelihood.

89
00:07:38.092 --> 00:07:41.062
Let's just look at the standard deviation.

90
00:07:45.927 --> 00:07:51.027
We can give the model initial values,
inits1 and

91
00:07:51.027 --> 00:07:55.773
this goes in as a function
that creates a list.

92
00:07:59.902 --> 00:08:03.220
We'll give an initial value for
the beta coefficients.

93
00:08:04.430 --> 00:08:05.878
There are two of them.

94
00:08:05.878 --> 00:08:11.814
So, let's draw two random normals.

95
00:08:11.814 --> 00:08:16.530
So drawing two random normals,
let's say, 0,

96
00:08:16.530 --> 00:08:21.034
and variance or standard deviation, 100.

97
00:08:25.567 --> 00:08:32.560
For initial value of the precision,
let's draw from a gamma distribution.

98
00:08:34.430 --> 00:08:35.610
We only need one draw.

99
00:08:36.870 --> 00:08:43.852
And let's just give it,
Shape 1 and rate 1.

100
00:08:43.852 --> 00:08:46.359
That's actually
an exponential distribution.

101
00:08:51.425 --> 00:08:57.076
So now that we've specified the data, told
it which parameters we want to monitor and

102
00:08:57.076 --> 00:09:00.620
given initial values for
the different parameters.

103
00:09:03.890 --> 00:09:09.115
We can specify the model itself,

104
00:09:09.115 --> 00:09:13.594
jags.model where we create

105
00:09:13.594 --> 00:09:19.571
a textConnection with mod1 string.

106
00:09:19.571 --> 00:09:25.225
mod1_string that we just specified.

107
00:09:25.225 --> 00:09:30.901
The data is data1_jags

108
00:09:30.901 --> 00:09:36.035
that we just created,

109
00:09:36.035 --> 00:09:39.825
data1_jags.

110
00:09:39.825 --> 00:09:44.355
We'll use the initial values
from our function inits1.

111
00:09:47.125 --> 00:09:50.890
And let's say we want to
run three different chains.

112
00:09:50.890 --> 00:09:54.920
That's another option to write
into this jags.model function.

113
00:09:56.070 --> 00:09:58.190
We want it to run three separate chains.

114
00:09:59.530 --> 00:10:05.370
In each chain it'll initialize
the chain with these random draws.

115
00:10:05.370 --> 00:10:07.990
So we'll have different
starting values for each chain.

116
00:10:09.570 --> 00:10:13.020
We'll initialize the model
by running all of this code.

117
00:10:16.982 --> 00:10:20.965
Initialization was essentially instant.

118
00:10:20.965 --> 00:10:24.609
And now that the model is initialized,

119
00:10:24.609 --> 00:10:29.801
let's give it a little bit of
a burn in period by updating

120
00:10:29.801 --> 00:10:34.889
the model for, let's say,
1,000 iterations.

121
00:10:34.889 --> 00:10:40.876
So it ran the model for 1,000 iterations
but it didn't keep the samples.

122
00:10:43.353 --> 00:10:48.185
Once we have the burn in period run,
let's create our actual

123
00:10:48.185 --> 00:10:53.790
posterior simulation that we're
going to keep for model inference.

124
00:10:53.790 --> 00:10:57.240
Mod1_sim is what we'll name it.

125
00:10:57.240 --> 00:11:00.890
It will come from coda.samples.

126
00:11:00.890 --> 00:11:03.750
We're going to create
coda.samples from the model.

127
00:11:06.732 --> 00:11:12.410
Model 1, where the variable names

128
00:11:13.580 --> 00:11:18.270
are the parameters that we created.

129
00:11:20.760 --> 00:11:27.651
And we'll run this for
let's say 5,000 iterations.

130
00:11:30.426 --> 00:11:32.439
We'll run this line to run the model.

131
00:11:34.282 --> 00:11:36.977
And since we ran three different chains,

132
00:11:36.977 --> 00:11:40.600
sometimes it's useful to
combine them into one chain.

133
00:11:41.680 --> 00:11:46.530
So we'll call that mod1_csim for
combined simulation.

134
00:11:47.770 --> 00:11:52.592
And we're going to do this by
stacking the matrices that

135
00:11:52.592 --> 00:11:55.988
contain the simulations themselves.

136
00:11:55.988 --> 00:12:02.180
Inside mod1.sim the simulations
are stored as matrices.

137
00:12:03.930 --> 00:12:11.040
To do this we need to use
the do.call function in R and rbind.

138
00:12:11.040 --> 00:12:16.910
Remember we used cbind earlier to
combine matrices along the columns.

139
00:12:16.910 --> 00:12:21.580
Now we're going to stack the matrices
vertically by combining them on

140
00:12:21.580 --> 00:12:26.560
the rows using mod1_sim.

141
00:12:26.560 --> 00:12:27.565
And run that line.