WEBVTT

1
00:00:00.000 --> 00:00:03.492
[MUSIC]

2
00:00:03.492 --> 00:00:07.501
>> Metropolis-Hastings is an algorithm
that allows us to sample from

3
00:00:07.501 --> 00:00:12.497
a generic probability distribution,
which we'll call our target distribution,

4
00:00:12.497 --> 00:00:16.260
even if we don't know
the normalizing constant.

5
00:00:16.260 --> 00:00:20.820
To do this, we construct and
sample from a Markov chain whose

6
00:00:20.820 --> 00:00:24.270
stationary distribution is the target
distribution that we're looking for.

7
00:00:25.380 --> 00:00:29.230
It consists of picking
an arbitrary starting value and

8
00:00:29.230 --> 00:00:33.810
then iteratively accepting or
rejecting candidate samples

9
00:00:33.810 --> 00:00:37.600
drawn from another distribution,
one that is easy to sample.

10
00:00:39.310 --> 00:00:43.500
Let's say we want to produce
samples from a target distribution.

11
00:00:43.500 --> 00:00:47.300
We're going to call it p of theta.

12
00:00:47.300 --> 00:00:51.950
But we only know it up to a normalizing
constant or up to proportionality.

13
00:00:53.240 --> 00:00:55.970
What we have is g of theta.

14
00:00:57.110 --> 00:00:59.780
So we don't know
the normalizing constant because

15
00:00:59.780 --> 00:01:02.490
perhaps this is difficult to integrate.

16
00:01:02.490 --> 00:01:04.430
So we only have g of theta to work with.

17
00:01:05.580 --> 00:01:09.136
The Metropolis Hastings Algorithm
will proceed as follows.

18
00:01:18.855 --> 00:01:23.470
The first step is to select
an initial value for theta.

19
00:01:32.716 --> 00:01:37.360
We're going to call it theta-naught.

20
00:01:37.360 --> 00:01:42.473
The next step is for
a large number of iterations,

21
00:01:42.473 --> 00:01:46.609
so for i from 1 up to some large number m,

22
00:01:46.609 --> 00:01:50.655
we're going to repeat the following.

23
00:01:56.111 --> 00:02:01.440
The first thing we're going
to do is draw a candidate.

24
00:02:04.670 --> 00:02:07.890
We'll call that theta-star
as our candidate.

25
00:02:07.890 --> 00:02:10.871
And we're going to draw this
from a proposal distribution.

26
00:02:14.366 --> 00:02:19.115
We're going to call the proposal
distribution q of theta-star,

27
00:02:19.115 --> 00:02:23.220
given the previous
iteration's value of theta.

28
00:02:23.220 --> 00:02:26.150
We'll take more about
this q distribution soon.

29
00:02:29.210 --> 00:02:33.050
The next step is to compute
the following ratio.

30
00:02:33.050 --> 00:02:34.290
We're going to call this alpha.

31
00:02:35.850 --> 00:02:41.143
It is this g function evaluated at

32
00:02:41.143 --> 00:02:46.230
the candidate divided by the distribution,
or

33
00:02:46.230 --> 00:02:53.770
the density here of q, evaluated at the
candidate given the previous iteration.

34
00:02:56.330 --> 00:03:01.900
And all of this will be divided by
g evaluated at the old iteration.

35
00:03:03.530 --> 00:03:07.980
That divided by q,
evaluated at the old iteration.

36
00:03:10.340 --> 00:03:13.470
Given the candidate value.

37
00:03:13.470 --> 00:03:16.928
If we rearrange this,

38
00:03:16.928 --> 00:03:22.024
it'll be g of the candidate times

39
00:03:22.024 --> 00:03:28.576
q of the previous value
given the candidate

40
00:03:28.576 --> 00:03:34.046
divided by g at the previous value.

41
00:03:34.046 --> 00:03:38.688
And q evaluated at the candidate,
given the previous value.

42
00:03:42.104 --> 00:03:47.670
The next step, once we've calculated
alpha, is to check alpha.

43
00:03:49.510 --> 00:03:54.916
If it's greater than or equal to 1,
then we're going to accept the candidate

44
00:03:57.732 --> 00:04:02.311
So we're going to accept
the candidate theta-star, and

45
00:04:02.311 --> 00:04:07.455
set Our current iteration of theta.

46
00:04:09.690 --> 00:04:11.580
So, that'll be i.

47
00:04:11.580 --> 00:04:14.530
Will be that candidate value.

48
00:04:14.530 --> 00:04:16.348
If alpha is between zero and 1,

49
00:04:21.261 --> 00:04:28.998
Then what we're going to
do is accept The candidate.

50
00:04:32.470 --> 00:04:37.182
And set Theta i

51
00:04:37.182 --> 00:04:42.293
equal to the candidate
with probability alpha.

52
00:04:48.256 --> 00:04:51.933
Then, the other possibility is to reject

53
00:04:55.706 --> 00:04:57.695
the candidate.

54
00:04:57.695 --> 00:05:02.873
And set The current

55
00:05:02.873 --> 00:05:07.460
iteration of i equal to
the previous iteration of i.

56
00:05:07.460 --> 00:05:14.922
And we do this with
probability 1minus alpha.

57
00:05:18.137 --> 00:05:23.833
These steps, b and c,
act as a correction since the proposal

58
00:05:23.833 --> 00:05:28.540
distribution q is not
the target distribution p.

59
00:05:28.540 --> 00:05:32.570
At each step in the chain,
we draw a candidate and

60
00:05:32.570 --> 00:05:36.979
decide whether to move the chain there or
to remain where we are.

61
00:05:38.010 --> 00:05:41.420
If the proposed move to
the candidate is advantageous,

62
00:05:41.420 --> 00:05:46.510
meaning if alpha is greater than 1,
we will move there for sure.

63
00:05:46.510 --> 00:05:50.260
If it is not advantageous,
we might still move there,

64
00:05:50.260 --> 00:05:52.620
but only with probability alpha.

65
00:05:54.490 --> 00:05:58.930
Since our decision to move to the
candidate only depends on where the chain

66
00:05:58.930 --> 00:06:03.710
currently is, this is a Markov chain.

67
00:06:03.710 --> 00:06:09.360
One careful choice we must make is with
the candidate generating distribution q.

68
00:06:10.540 --> 00:06:14.375
It may or may not depend on the previous
iteration's value of theta.

69
00:06:15.830 --> 00:06:19.570
One example where it doesn't
depend on the previous value

70
00:06:19.570 --> 00:06:24.420
would be if q of theta-star is
always the same distribution.

71
00:06:25.530 --> 00:06:26.780
If we take this option,

72
00:06:26.780 --> 00:06:32.280
q of theta should be similar to
p of theta, to approximate it.

73
00:06:33.590 --> 00:06:38.700
Another popular option, one that does
depend on the previous iteration,

74
00:06:38.700 --> 00:06:42.640
is called random walk Metropolis-Hastings.

75
00:06:42.640 --> 00:06:48.570
Here, the proposal distribution is
centered on the previous iteration.

76
00:06:48.570 --> 00:06:52.470
For instance, it might be a normal
distribution where the mean

77
00:06:52.470 --> 00:06:57.030
is our previous iteration theta i minus 1.

78
00:06:57.030 --> 00:07:01.820
Because the normal distribution
is symmetric around its mean,

79
00:07:01.820 --> 00:07:04.880
this example comes with
another really nice advantage.

80
00:07:06.070 --> 00:07:11.020
This q evaluated at the candidate
given the mean here,

81
00:07:12.710 --> 00:07:17.060
the density value for this q will
be equal to this density value of

82
00:07:17.060 --> 00:07:21.020
the old value where
the mean is the candidate.

83
00:07:22.290 --> 00:07:27.550
This causes these two qs to cancel
out when we calculate alpha.

84
00:07:27.550 --> 00:07:32.060
So, in random walk Metropolis-Hastings,
where the candidate is drawn from a normal

85
00:07:32.060 --> 00:07:36.450
distribution where the mean is
the previous iteration's value and

86
00:07:36.450 --> 00:07:41.050
we use a constant variance in that normal
distribution, the acceptance ratio,

87
00:07:41.050 --> 00:07:43.320
alpha, will be really easy.

88
00:07:43.320 --> 00:07:47.840
It'll just be g at the evaluated at
the candidate divided by g evaluated

89
00:07:47.840 --> 00:07:49.658
at the previous iteration.

90
00:07:52.315 --> 00:07:56.260
Clearly, not all candidate
draws are accepted.

91
00:07:56.260 --> 00:08:01.720
So, our Markov chain sometimes stays where
it is, possibly for many iterations.

92
00:08:02.850 --> 00:08:06.110
How often you want the chain
to accept candidates

93
00:08:06.110 --> 00:08:09.350
depends on the type of algorithm you use.

94
00:08:09.350 --> 00:08:14.680
If you approximate p with q and
always draw candidates

95
00:08:14.680 --> 00:08:19.840
from that distribution, accepting
candidates often is a good thing.

96
00:08:19.840 --> 00:08:22.780
It means that q is approximating p well.

97
00:08:23.916 --> 00:08:29.610
However, you still may want to have
q have a larger variance than p,

98
00:08:29.610 --> 00:08:34.280
and see some rejection of
candidates to be as an assurance

99
00:08:34.280 --> 00:08:36.610
that q is covering the space well.

100
00:08:38.770 --> 00:08:42.150
As we'll see in coming examples,
a high acceptance rate for

101
00:08:42.150 --> 00:08:47.070
random walk Metropolis-Hastings
samplers is not a good thing.

102
00:08:47.070 --> 00:08:53.630
If the random walk is taking too small of
steps, it will accept candidates often,

103
00:08:53.630 --> 00:08:57.279
but will take a very long time to fully
explore the posterior distribution.

104
00:08:59.340 --> 00:09:04.050
If the random walk is taking too large of
steps, many of its proposals will have

105
00:09:04.050 --> 00:09:07.520
low probability and
the acceptance rate will be low.

106
00:09:07.520 --> 00:09:10.060
That will cause us to
waste many of the draws.

107
00:09:11.180 --> 00:09:16.305
Ideally, a random walk sampler should
accept somewhere between 23 and

108
00:09:16.305 --> 00:09:18.549
50% of the candidates proposed.

109
00:09:19.780 --> 00:09:24.090
In the next segment we're going
to demonstrate this algorithm

110
00:09:24.090 --> 00:09:28.770
in the discrete case, where we can show
mathematically that the Markov chain

111
00:09:28.770 --> 00:09:30.880
converges to the target distribution.

112
00:09:32.450 --> 00:09:36.767
In the following segment after that we
will demonstrate coding a random walk

113
00:09:36.767 --> 00:09:41.151
Metropolis-Hastings algorithm in R to
solve one of the problems from the end

114
00:09:41.151 --> 00:09:42.038
of lesson two.

115
00:09:42.038 --> 00:09:48.559
[MUSIC]