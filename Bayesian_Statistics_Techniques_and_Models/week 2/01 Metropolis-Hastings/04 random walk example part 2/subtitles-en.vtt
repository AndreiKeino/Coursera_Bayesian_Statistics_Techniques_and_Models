WEBVTT

1
00:00:01.300 --> 00:00:03.060
Now we're ready to set up the problem.

2
00:00:04.530 --> 00:00:10.260
I have copied here a vector of
the data values and we've called it y.

3
00:00:11.840 --> 00:00:18.670
So what we need is to calculate y
bar which is the sample mean of y.

4
00:00:21.110 --> 00:00:27.704
We also need to run y so that
the values are saved in our R session.

5
00:00:30.401 --> 00:00:35.680
And now we can run this line to
calculate y bar, the mean of y.

6
00:00:36.990 --> 00:00:40.394
We also need n, which is the length of y.

7
00:00:42.336 --> 00:00:49.303
We'll run that and
just to check it, it is 10.

8
00:00:49.303 --> 00:00:53.653
And as part of our analysis let's just
look at a histogram of these data points.

9
00:00:55.452 --> 00:01:01.141
So we'll do a histogram of y, where again,
we want the frequency to be false,

10
00:01:01.141 --> 00:01:06.850
so that we're looking, instead of
counts we're looking at probabilities.

11
00:01:08.370 --> 00:01:12.280
And we want the x axis

12
00:01:14.450 --> 00:01:18.534
to be between the values of negative 1 and

13
00:01:18.534 --> 00:01:23.630
3 just so that we have enough
view on both sides of the data.

14
00:01:24.670 --> 00:01:26.240
Let's run this histogram.

15
00:01:29.230 --> 00:01:30.660
Here's what the data look like.

16
00:01:32.350 --> 00:01:34.870
Let's also add the individual data points.

17
00:01:36.300 --> 00:01:38.190
We can do that with the points function.

18
00:01:40.318 --> 00:01:42.530
Where we add the y vector.

19
00:01:44.760 --> 00:01:46.660
And we want, on the y axis,

20
00:01:46.660 --> 00:01:52.490
we want these to appear on 0 so
that they appear along the x axis.

21
00:01:52.490 --> 00:01:59.010
So we're going to repeat 0 n times for
our y coordinant.

22
00:02:00.610 --> 00:02:03.990
And we'll actually just run this.

23
00:02:05.700 --> 00:02:08.160
So that adds the actual data points.

24
00:02:08.160 --> 00:02:09.330
So we can see where they are.

25
00:02:10.420 --> 00:02:15.740
We can also add y bar, the sample
mean to see where that's located.

26
00:02:16.970 --> 00:02:23.675
Again, we'll use points,
and this time y bar and 0.

27
00:02:23.675 --> 00:02:27.116
And to distinguish it
from the data points,

28
00:02:27.116 --> 00:02:31.820
let's use pch-19 that
creates a field circle.

29
00:02:31.820 --> 00:02:37.230
So when I run this,
it adds y bar, the sample mean.

30
00:02:37.230 --> 00:02:39.940
So we can see where the sample
mean of our data are.

31
00:02:41.450 --> 00:02:45.490
Let's add the plot of our
prior distribution for

32
00:02:45.490 --> 00:02:49.790
the mean to see how it
compares with our data.

33
00:02:51.140 --> 00:02:57.090
We can do with the curve
function where we're going to use

34
00:02:57.090 --> 00:03:01.420
the density of a t distribution dt for

35
00:03:01.420 --> 00:03:07.000
generic x variable, which we haven't
define is just the x axis in the plot.

36
00:03:08.530 --> 00:03:12.890
And our t distribution
has degree of freedom 1.

37
00:03:12.890 --> 00:03:17.950
By default that already has location 0 and
scale 1.

38
00:03:17.950 --> 00:03:19.610
To distinguish it from the histogram,

39
00:03:19.610 --> 00:03:24.030
let's give it a different line type,
lty=2.

40
00:03:24.030 --> 00:03:26.556
That will cause it to be a dashed line.

41
00:03:26.556 --> 00:03:33.410
And then of course, add=true so that it
adds this plot on top of the previous one.

42
00:03:33.410 --> 00:03:34.250
Let's run this curve.

43
00:03:35.960 --> 00:03:38.280
Here's our prior distribution for mu.

44
00:03:39.520 --> 00:03:43.830
As you can see, there's a little bit
of a discrepancy between our prior for

45
00:03:43.830 --> 00:03:47.200
mu, and what the data says mu should be.

46
00:03:47.200 --> 00:03:52.515
The prior has most of its probability
mass in this region here, near 0.

47
00:03:53.590 --> 00:03:58.325
However, the data suggest that
the mean is up here near 1.

48
00:03:59.740 --> 00:04:04.883
We expect that the posterior
distribution from mu will

49
00:04:04.883 --> 00:04:10.149
have a mean as a compromise
somewhere between 0 and 1.

50
00:04:10.149 --> 00:04:13.741
We're now ready to run the sampler
that we've just coded up.

51
00:04:15.958 --> 00:04:20.429
I've already entered the LG
function into the R session but

52
00:04:20.429 --> 00:04:24.920
we still need to load
the Metropolis-Hastings function.

53
00:04:24.920 --> 00:04:29.508
So I'm going to press Cmd+Return to
enter that function into the console.

54
00:04:32.338 --> 00:04:34.178
And now we have the data.

55
00:04:34.178 --> 00:04:37.490
We have y bar and we have n.

56
00:04:37.490 --> 00:04:40.434
So let's move down here and
start sampling.

57
00:04:41.932 --> 00:04:47.150
We'll collect our posterior samples and
a variable that we'll call post.

58
00:04:48.150 --> 00:04:49.293
Referring to posterior.

59
00:04:51.966 --> 00:04:57.198
It would be the result of calling
our Metropolis-Hastings function,

60
00:04:57.198 --> 00:05:00.523
where we need to give
it these six arguments.

61
00:05:00.523 --> 00:05:06.796
n, which we've defined, y bar which
is the sample mean of the data.

62
00:05:10.068 --> 00:05:13.770
N_iter, which is the number of iterations.

63
00:05:13.770 --> 00:05:17.390
We've decided we want to do
1,000 iterations of our sampler.

64
00:05:18.860 --> 00:05:22.760
There's also a shorthand for
this which we can do, 1e3.

65
00:05:24.200 --> 00:05:26.320
That's 10 to the 3rd power.

66
00:05:27.540 --> 00:05:34.870
Let's initialize mu at 0.

67
00:05:34.870 --> 00:05:40.990
We'll use candidate generating
standard deviation equal to 3.

68
00:05:40.990 --> 00:05:45.910
Before we start sampling,
let's set the random generator seed.

69
00:05:48.808 --> 00:05:51.568
So that we can reproduce
our results if we need to.

70
00:05:53.756 --> 00:05:58.660
So first thing we'll run is set.seed.

71
00:05:58.660 --> 00:06:03.560
And then we'll run our
Metropolis-Hastings function

72
00:06:03.560 --> 00:06:05.170
using the parameters that we gave it.

73
00:06:07.810 --> 00:06:12.680
If we've saved some object, and
we're curious what's inside of it,

74
00:06:12.680 --> 00:06:16.450
we can use the str function,
str function in R.

75
00:06:17.850 --> 00:06:21.770
And put the object in there to
find out what's in the object.

76
00:06:24.000 --> 00:06:27.850
So the structure of post is a list.

77
00:06:27.850 --> 00:06:29.850
Just like we had our function output.

78
00:06:31.510 --> 00:06:37.270
It's a list containing 1,000
iterations of our new variable.

79
00:06:37.270 --> 00:06:39.660
And it tells us our acceptance rate.

80
00:06:39.660 --> 00:06:44.513
Which in our case, for
this run, was about 12%.

81
00:06:44.513 --> 00:06:47.633
To help us explore
posterior distributions,

82
00:06:47.633 --> 00:06:50.435
we're going to use the coda package in R.

83
00:06:53.287 --> 00:06:55.120
So let's load the coda package.

84
00:06:58.290 --> 00:07:02.630
This R session does not
currently have coda installed.

85
00:07:02.630 --> 00:07:06.070
So first we're going to need
to install the package.

86
00:07:06.070 --> 00:07:08.100
install.packages.

87
00:07:11.082 --> 00:07:13.234
And we'll install Coda.

88
00:07:13.234 --> 00:07:14.714
So we'll run that line.

89
00:07:19.091 --> 00:07:21.244
When it installs a package,

90
00:07:21.244 --> 00:07:26.940
it asks us which CRAN mirror we
want to download the package from.

91
00:07:26.940 --> 00:07:30.471
We're in California, so
let's select California here.

92
00:07:34.418 --> 00:07:39.060
Great, so hopefully now
library coda will work for us.

93
00:07:39.060 --> 00:07:41.980
Let's run this line, perfect.

94
00:07:41.980 --> 00:07:46.980
It worked, now that we've
loaded the coda library we can

95
00:07:46.980 --> 00:07:52.800
use the trace plot function,
which I'll describe in just a moment.

96
00:07:52.800 --> 00:07:57.680
The trace plot function requires
that we pass it an MCMC object.

97
00:07:57.680 --> 00:08:00.250
Right now post is only a list.

98
00:08:00.250 --> 00:08:07.844
So let's change post into an MCMC object
by typing as.mcmc, that's also a function.

99
00:08:10.193 --> 00:08:16.090
Post, specifically we want to
look at the trace plot of mu.

100
00:08:16.090 --> 00:08:19.110
So this is the mu vector in the post.

101
00:08:19.110 --> 00:08:20.850
Our list of posts right here.

102
00:08:21.940 --> 00:08:23.851
Let's run this line and see what happens.

103
00:08:27.590 --> 00:08:28.902
Here's our trace plot.

104
00:08:32.320 --> 00:08:38.550
So this is called a trace plot because it
shows the history of our Markov chain.

105
00:08:38.550 --> 00:08:42.470
And it gives us some basic feedback
about whether the chain has reached

106
00:08:42.470 --> 00:08:44.070
the stationary distribution.

107
00:08:45.370 --> 00:08:50.006
It appears that our proposal step
size in this case was a little bit

108
00:08:50.006 --> 00:08:54.153
too large because we had
an acceptance rate below 23%.

109
00:08:54.153 --> 00:08:57.000
It was somewhere,
it was more like 12% I think in our case.

110
00:08:58.790 --> 00:08:59.593
Yeah, here it is.

111
00:08:59.593 --> 00:09:01.014
It was 12%.

112
00:09:02.400 --> 00:09:04.740
You can see that in the trace plot.

113
00:09:04.740 --> 00:09:10.670
As the sampler was moving along, for
example in these early iterations,

114
00:09:10.670 --> 00:09:16.790
mu got stuck at a single value for quite
a long time, it happened again over here.

115
00:09:16.790 --> 00:09:17.790
So in other words,

116
00:09:17.790 --> 00:09:22.210
our Markov chain is not moving around
as much as we would like it to.

117
00:09:22.210 --> 00:09:25.580
We want to increase the acceptance rate.

118
00:09:26.700 --> 00:09:28.730
To change the acceptance rate,

119
00:09:28.730 --> 00:09:34.090
we need to rerun the sampler with
a different candidate standard deviation.

120
00:09:35.530 --> 00:09:39.640
Usually, if we increase
the variability in the distribution

121
00:09:39.640 --> 00:09:44.560
that creates the candidates,
that will decrease the acceptance rate.

122
00:09:45.910 --> 00:09:51.420
We want to increase the acceptance rate,
so we want a smaller standard deviation.

123
00:09:51.420 --> 00:09:55.190
Let's reduce it to say, 0.05.

124
00:09:55.190 --> 00:09:57.919
Let's make it really small and
see what happens.

125
00:10:00.095 --> 00:10:05.477
So if we run the Metropolis-Hastings
function and reassign it to post,

126
00:10:05.477 --> 00:10:10.110
it'll override our old samples,
but that's okay.

127
00:10:10.110 --> 00:10:12.288
We still want to initialize mu at 0.

128
00:10:12.288 --> 00:10:15.760
And we'll still run it for
1,000 iterations.

129
00:10:17.000 --> 00:10:18.100
Let's run this line of code.

130
00:10:20.060 --> 00:10:22.190
And let's look at
the structure of post again.

131
00:10:23.680 --> 00:10:26.652
So we have 1,000 samples, but

132
00:10:26.652 --> 00:10:31.417
this time the acceptance rate was 94,
almost 95%.

133
00:10:31.417 --> 00:10:39.810
If we look at the trace plot for
this chain, we see the opposite behavior.

134
00:10:39.810 --> 00:10:44.439
Notice that the step sizes, because
the candidate generating function have

135
00:10:44.439 --> 00:10:47.470
a small variance,
the step sizes are small.

136
00:10:47.470 --> 00:10:49.198
And so the chain wanders.

137
00:10:51.127 --> 00:10:53.780
Here the step sizes are too small.

138
00:10:53.780 --> 00:10:57.940
It's going to take the chain a long time
to explore the posterior distribution.

139
00:10:59.400 --> 00:11:03.521
We can see that because the acceptance
rate for our random walk is too high.

140
00:11:05.995 --> 00:11:10.090
The first candidate standard
deviation rate we tried was 3,

141
00:11:10.090 --> 00:11:15.540
we over corrected by going down to 0.05,
so let's try something in between.

142
00:11:15.540 --> 00:11:22.640
Let's try 0.9,
that's a compromise from the two values.

143
00:11:22.640 --> 00:11:26.910
Let's run this, and
now look at our acceptance rate.

144
00:11:28.050 --> 00:11:32.800
Okay, our acceptance rate is down to 0.38,
that's probably good.

145
00:11:32.800 --> 00:11:37.205
We usually are looking for
an acceptance rate between 0.23 and

146
00:11:37.205 --> 00:11:42.640
0.5 when we're working with a random
walk Metropolis-Hastings algorithm.

147
00:11:44.990 --> 00:11:47.400
So let's look at the trace plot for this.

148
00:11:47.400 --> 00:11:48.871
I'll run the trace plot command.

149
00:11:51.438 --> 00:11:53.850
This trace plot looks much better.

150
00:11:53.850 --> 00:11:56.640
It's exploring the posterior distribution.

151
00:11:56.640 --> 00:11:59.160
It gets stuck every once in a while but
that's okay.

152
00:12:00.170 --> 00:12:01.240
It looks pretty good.

153
00:12:02.950 --> 00:12:07.730
Now let's see what happens
if we change our initial

154
00:12:07.730 --> 00:12:12.188
value to something way
far away from the truth.

155
00:12:12.188 --> 00:12:20.988
So let's try an initial
value of mu=30 We run that.

156
00:12:23.022 --> 00:12:28.271
And look at the acceptance,
we still get 0.38, that's good.

157
00:12:28.271 --> 00:12:30.740
And let's look at our trace plot.

158
00:12:30.740 --> 00:12:31.760
We'll run this line.

159
00:12:34.800 --> 00:12:41.070
Here we can see the effect of
using a crazy initial value.

160
00:12:41.070 --> 00:12:44.920
We started mu way up here at 30,
and it took maybe

161
00:12:46.080 --> 00:12:50.860
almost 100 iterations to find
the stationary distribution.

162
00:12:50.860 --> 00:12:54.990
It was wondering for quite a long time,
and then once it hits the stationary

163
00:12:54.990 --> 00:12:59.930
distribution, it bounced around
inside the stationary distribution.

164
00:12:59.930 --> 00:13:04.660
But it looks like after 100 iterations,
we've succeeded.

165
00:13:04.660 --> 00:13:08.840
So if we discard or
throw away the first 100 or so values,

166
00:13:08.840 --> 00:13:13.640
it looks like the rest of the samples do
come from the stationary distribution,

167
00:13:13.640 --> 00:13:16.330
which is the posterior
distribution we wanted.

168
00:13:17.670 --> 00:13:22.280
Now let's plot the posterior
density against the prior

169
00:13:22.280 --> 00:13:26.350
to see how the data updated
our belief about mu.

170
00:13:28.210 --> 00:13:32.920
The first thing we'll do is get
rid of those first 200 iterations

171
00:13:32.920 --> 00:13:35.380
while the chain was still exploring.

172
00:13:35.380 --> 00:13:42.810
We're going to call this a new
object in our list called post.

173
00:13:44.050 --> 00:13:46.695
And it'll be mu_keep.

174
00:13:46.695 --> 00:13:49.550
It'll be the iterations of
mu that we want to keep.

175
00:13:50.550 --> 00:13:55.910
It'll come from the mu
object in our list for

176
00:13:55.910 --> 00:13:59.570
post where we get rid of.

177
00:13:59.570 --> 00:14:03.280
We're going to discard
iterations 1 through,

178
00:14:04.820 --> 00:14:08.550
let's just get rid of the first 100, okay.

179
00:14:10.610 --> 00:14:15.295
So this says,
we're going to index the mu vector

180
00:14:15.295 --> 00:14:19.426
by taking away the first 100 elements.

181
00:14:19.426 --> 00:14:22.040
Let's run that line, it saves.

182
00:14:23.150 --> 00:14:26.610
So that we can look at
the structure of post,

183
00:14:27.740 --> 00:14:31.850
we now have mu.keep or mu_keep.

184
00:14:31.850 --> 00:14:36.060
So let's plot a density estimate so

185
00:14:36.060 --> 00:14:40.890
it's plot (density),

186
00:14:40.890 --> 00:14:47.934
a density estimate of our posterior
draws from mu, post$mu_keep.

187
00:14:49.180 --> 00:14:54.000
Let's use the same limits to
our x axis that we did before.

188
00:14:54.000 --> 00:15:00.005
So that would be xlim going
from negative 1 to 3.

189
00:15:00.005 --> 00:15:02.660
And let's take a look at this plot.

190
00:15:02.660 --> 00:15:03.250
Let's run this.

191
00:15:04.600 --> 00:15:09.220
So this is a density estimate
of the posterior distribution

192
00:15:09.220 --> 00:15:13.182
based on the 10 data points.

193
00:15:13.182 --> 00:15:18.190
Again, let's compare this to the data and
the prior.

194
00:15:18.190 --> 00:15:19.440
First, let's draw the prior.

195
00:15:20.560 --> 00:15:24.800
Again, we can reuse our old code up here,

196
00:15:24.800 --> 00:15:30.475
where we drew the prior for
the density of the t distribution with

197
00:15:30.475 --> 00:15:35.617
1 degree of freedom, where add=true so
that we would write it over the same plot.

198
00:15:35.617 --> 00:15:37.036
Let's run this line.

199
00:15:39.989 --> 00:15:42.855
So here is our prior distribution for
mu and

200
00:15:42.855 --> 00:15:47.910
then based on the data,here is our
posterior distribution for mu.

201
00:15:47.910 --> 00:15:50.040
Let's also add the sample mean.

202
00:15:52.160 --> 00:15:55.340
So this is where the data
said mu should be.

203
00:15:55.340 --> 00:15:57.920
This is where the prior said mu should be.

204
00:15:57.920 --> 00:16:03.470
The posterior distribution does
look like a compromise between

205
00:16:03.470 --> 00:16:05.340
the prior distribution and the data.

206
00:16:06.660 --> 00:16:11.070
This results we've obtained
are encouraging but they are preliminary.

207
00:16:11.070 --> 00:16:16.130
We still need to investigate a little
more formally whether our Markov chain

208
00:16:16.130 --> 00:16:18.940
has actually converged to
the stationery distribution,

209
00:16:18.940 --> 00:16:22.110
and we're going to explore
this in a future lesson.

210
00:16:23.900 --> 00:16:28.310
Creating posterior samples using
a Metropolis-Hastings algorithm

211
00:16:28.310 --> 00:16:32.170
can be time consuming and
require a lot of fine tuning

212
00:16:32.170 --> 00:16:35.100
like we did with our candidate
standard deviation here.

213
00:16:36.620 --> 00:16:41.470
The good news is that we can rely on
software to do most of the work for us.

214
00:16:41.470 --> 00:16:43.362
So in the next couple of videos,

215
00:16:43.362 --> 00:16:47.926
we're going to introduce a program that
will make posterior sampling easy.