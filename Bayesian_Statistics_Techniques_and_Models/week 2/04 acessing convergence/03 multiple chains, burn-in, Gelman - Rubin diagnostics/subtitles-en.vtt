WEBVTT

1
00:00:00.740 --> 00:00:05.080
We've seen in previous examples
how the initial value of the chain

2
00:00:05.080 --> 00:00:08.180
can affect how quickly
the chain converges.

3
00:00:11.050 --> 00:00:16.020
If our initial value is far from
the bulk of the posterior distribution,

4
00:00:16.020 --> 00:00:21.300
then it may take a while for the chain to
travel there as we saw in this example.

5
00:00:21.300 --> 00:00:24.000
Where the posterior mean is close to 1,

6
00:00:24.000 --> 00:00:27.440
but we started the chain
way out here at 10.

7
00:00:27.440 --> 00:00:32.120
It took a long time for
the chain to wander to the right area and

8
00:00:32.120 --> 00:00:34.640
start exploring that
posterior distribution.

9
00:00:35.810 --> 00:00:37.460
Clearly, the first 100 or so

10
00:00:37.460 --> 00:00:42.790
iterations do not reflect draws
from this stationary distribution.

11
00:00:42.790 --> 00:00:47.840
So they should be discarded before we use
this chain for Monte Carlo estimates.

12
00:00:48.940 --> 00:00:51.760
This is called the burn in period.

13
00:00:51.760 --> 00:00:55.650
You should always discard early iterations

14
00:00:55.650 --> 00:00:59.620
that do not appear to be coming
from the stationary distribution.

15
00:00:59.620 --> 00:01:03.535
Even if the chain appears
to have converged early on,

16
00:01:03.535 --> 00:01:07.541
it is a safer practice to
discard an initial burn-in.

17
00:01:11.413 --> 00:01:16.228
If we want to be more confident that we
have converged to this true stationary

18
00:01:16.228 --> 00:01:19.464
distribution, we can
simulate multiple chains,

19
00:01:19.464 --> 00:01:22.640
each with a different starting value.

20
00:01:22.640 --> 00:01:24.369
Let's run that simulation.

21
00:01:27.356 --> 00:01:30.660
Where post1 has a specific starting value.

22
00:01:30.660 --> 00:01:36.658
Post2 has a different starting value and
so forth down to posterior5.

23
00:01:36.658 --> 00:01:41.507
I'm going to combine all of
these chains into one MCMC

24
00:01:41.507 --> 00:01:44.754
object using the MCMC list function

25
00:01:49.831 --> 00:01:54.130
Now, let's look at the trace plot for
this multi-chain simulation.

26
00:01:55.360 --> 00:01:59.780
We started each of this chains at
very different initial values, but

27
00:01:59.780 --> 00:02:04.806
as you can see, as we progress through
the iterations, after about 100

28
00:02:04.806 --> 00:02:10.960
maybe 200 iterations, these chains have
all converged to the same location.

29
00:02:12.660 --> 00:02:16.354
This increases our confidence
that what we have found is

30
00:02:16.354 --> 00:02:20.455
in fact the stationary distribution
that we are looking for.

31
00:02:24.295 --> 00:02:30.450
Trace plots provide an informal diagnostic
for the convergence of our chains.

32
00:02:30.450 --> 00:02:35.950
We can back up these visual results
with the Gelman and Rubin diagnostic.

33
00:02:37.240 --> 00:02:41.670
This diagnostic requires
that we fit multiple chains.

34
00:02:41.670 --> 00:02:45.050
It calculates the variability
within chains and

35
00:02:45.050 --> 00:02:48.350
compares that to the variability
between the chains.

36
00:02:49.430 --> 00:02:53.660
First, we need to combine
the chains all into one object here

37
00:02:53.660 --> 00:02:58.240
with mcmc.list function and

38
00:02:58.240 --> 00:03:04.060
we'll start a new script and
call the diagnostic.

39
00:03:04.060 --> 00:03:09.875
It's part of the coda package,
and it's called gelman.diag for

40
00:03:09.875 --> 00:03:13.030
that object that we just created.

41
00:03:15.570 --> 00:03:20.000
Again, the diagnostic compares
the variability within the chains

42
00:03:20.000 --> 00:03:23.040
to the variability between the chains.

43
00:03:23.040 --> 00:03:26.890
If all chains have converged
to the stationary distribution

44
00:03:26.890 --> 00:03:32.010
the variability between the chains
should be relatively small.

45
00:03:32.010 --> 00:03:37.000
And this potential scale reduction factor,
which is the statistic

46
00:03:37.000 --> 00:03:42.600
computed by this diagnostic,
should be a small number close to 1.

47
00:03:42.600 --> 00:03:45.350
If values are much higher than 1,

48
00:03:45.350 --> 00:03:48.940
then we would conclude that
the chains have not yet converged.

49
00:03:50.690 --> 00:03:55.054
We can also create a plot
to go with this diagnostic,

50
00:03:55.054 --> 00:03:59.728
it's called gelman.plot,
for our combined chains.

51
00:04:05.456 --> 00:04:10.860
And it shows us how the shrinkage factor
changes as we add iterations to the chain.

52
00:04:12.430 --> 00:04:18.600
From this plot, we can see that if
we only use the first 50 iterations,

53
00:04:18.600 --> 00:04:23.460
then the potential scale reduction factor,
or the shrink factor,

54
00:04:23.460 --> 00:04:28.370
would be close to ten indicating
that the chains have not converged.

55
00:04:29.520 --> 00:04:34.700
But once we're passed about 300
iterations, the shrink factor is

56
00:04:34.700 --> 00:04:39.810
essentially one indicating that by then,
we have probably reached convergence.

57
00:04:41.300 --> 00:04:46.170
Of course we shouldn't stop sampling as
soon as we reach convergent, instead this

58
00:04:46.170 --> 00:04:51.020
is where we should begin saving our
samples for Monte Carlo estimation.

59
00:04:52.810 --> 00:04:55.960
To get more information
on this diagnostics,

60
00:04:55.960 --> 00:05:03.950
we can type ?gelman.diag to access
the documentation for this function.

61
00:05:03.950 --> 00:05:06.680
Again if we want to learn
more about the Gelman and

62
00:05:06.680 --> 00:05:11.180
Rubin convergence diagnostic,
this would be the place to start.

63
00:05:11.180 --> 00:05:16.189
It describes the diagnostic,
describes the function, talks a little

64
00:05:16.189 --> 00:05:21.625
bit about the theory underlying this
diagnostic, and gives us the sources.

65
00:05:25.305 --> 00:05:29.933
If we are reasonably confident that
our Markov chain has converged,

66
00:05:29.933 --> 00:05:34.796
then we can go ahead and treat it as
though it were a Monte Carlo sample from

67
00:05:34.796 --> 00:05:40.020
the posterior distribution, albeit
with a smaller effective sample size.

68
00:05:41.640 --> 00:05:45.550
That means that we can use
the techniques from lesson three

69
00:05:45.550 --> 00:05:50.420
to calculate posterior quantities
like the posterior mean and

70
00:05:50.420 --> 00:05:53.570
the posterior intervals
from the samples directly.

71
00:05:55.050 --> 00:05:59.313
Let's return to the first example where
we have the chain that appeared to have

72
00:05:59.313 --> 00:06:03.461
converged, to remind ourselves let's
look at the trace plot for it again.

73
00:06:05.947 --> 00:06:11.060
We have 4,000 iterations of a chain that
appears that is has already converged.

74
00:06:12.240 --> 00:06:19.350
But, to be safe, let's discard
the first 1,000 iterations as burn-in.

75
00:06:19.350 --> 00:06:26.671
So the number of burn-in iterations,
nburn will be 1000.

76
00:06:26.671 --> 00:06:27.670
Let's read that in.

77
00:06:29.130 --> 00:06:32.580
And let's create a set of
samples that we want to keep.

78
00:06:32.580 --> 00:06:36.526
We'll add this onto our list for

79
00:06:36.526 --> 00:06:40.481
post0, and call it mu_keep.

80
00:06:42.113 --> 00:06:47.190
It will come from the original post0 mus.

81
00:06:47.190 --> 00:06:54.516
But now we're going to discard the first
1000 iterations, 1 through nburn.

82
00:06:57.204 --> 00:06:59.935
Let's look at the structure now of post0

83
00:07:04.063 --> 00:07:07.572
And now we have the samples of
mu that we're going to keep.

84
00:07:07.572 --> 00:07:09.940
There are 3,000 of these samples.

85
00:07:11.440 --> 00:07:16.209
Since we're confident that this has
reached the stationary distribution,

86
00:07:16.209 --> 00:07:19.380
we can be confident In
our Monte Carlo estimate.

87
00:07:19.380 --> 00:07:23.125
So let's look at
the summary of this object.

88
00:07:26.155 --> 00:07:30.560
Of mu_keep.

89
00:07:30.560 --> 00:07:32.275
We've seen this before.

90
00:07:32.275 --> 00:07:35.895
It provides a brief summary
of our inferences for

91
00:07:35.895 --> 00:07:39.815
the mean mu,
that parameter we've been estimating.

92
00:07:39.815 --> 00:07:44.503
It has mean close to 0.9 as we
have seen in previous examples.

93
00:07:44.503 --> 00:07:49.124
Standard deviation 0.3 and
has these different quintiles.

94
00:07:49.124 --> 00:07:54.840
For example,
the 0.025 quintile is this number and

95
00:07:54.840 --> 00:07:58.560
the 0.97 quintile is this number.

96
00:07:58.560 --> 00:08:03.357
So if we wanted to create
a posterior interval for mu,

97
00:08:03.357 --> 00:08:08.155
we could say that the posterior
probability that mu is

98
00:08:08.155 --> 00:08:12.650
between this number and
this number is 0.95.

99
00:08:12.650 --> 00:08:18.670
We can also compute posterior
probabilities of different hypotheses.

100
00:08:18.670 --> 00:08:19.720
For example,

101
00:08:19.720 --> 00:08:24.175
if we want to know the posterior
probability that mu is greater than 1.

102
00:08:26.450 --> 00:08:29.022
We'll just take the mean of an indicator

103
00:08:36.329 --> 00:08:41.161
And we're going to count how many of
these samples are greater than 1.

104
00:08:44.154 --> 00:08:51.059
Our posterior probability is about
0.35 that mu is greater than 1